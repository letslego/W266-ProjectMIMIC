{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import *\n",
    "import re\n",
    "from tensorflow.contrib import learn\n",
    "import sys, os\n",
    "import tensorflow as tf\n",
    "import cnn_model\n",
    "import utils\n",
    "\n",
    "from sklearn.metrics import label_ranking_loss\n",
    "from sklearn.metrics import f1_score\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Sources:\n",
    "http://ruder.io/deep-learning-nlp-best-practices/index.html#classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the dataset:  45837\n"
     ]
    }
   ],
   "source": [
    "#with open('../../../psql_files/disch_notes_all_icd9.csv', 'rb') as csvfile:\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "with open('../baseline/psql_files/dis_notes_icd9.csv', 'rb') as csvfile:\n",
    "    discharge_notes_reader = csv.reader(csvfile)\n",
    "    discharge_notes_list = list(discharge_notes_reader)    \n",
    "random.shuffle(discharge_notes_list)\n",
    "\n",
    "print \"Number of records in the dataset: \", len (discharge_notes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will take only 10,000 records to compare with NN baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting for 1,000 just for programming\n",
    "number_records = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of discharge clinical notes:  1000\n"
     ]
    }
   ],
   "source": [
    "discharge_notes_icd9 = np.asarray(discharge_notes_list[0:number_records])\n",
    "print 'Number of discharge clinical notes: ', len(discharge_notes_icd9)\n",
    "discharge_notes= discharge_notes_icd9[:,3]\n",
    "discharge_labels = discharge_notes_icd9[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats about Notes  (TODO:)\n",
    "* vocabulary of size\n",
    "* find out notes that are too large, outliers to take out (otherwise the embeddings will pad a lot of zeroes to the other note-vectors("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting icd9 labels to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transforming list of icd_codes into a vector\n",
    "def get_icd9_array(icd9_codes):\n",
    "    icd9_index_array = [0]*len(unique_icd9_codes)\n",
    "    for icd9_code in icd9_codes.split():\n",
    "        index = icd9_to_id [icd9_code]\n",
    "        icd9_index_array[index] = 1\n",
    "    return icd9_index_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'4019': 468, '4280': 294, '41401': 281, '42731': 253, '5849': 215, '2724': 202, '25000': 202, '51881': 185, '5990': 156, '2720': 145, '53081': 143, '2859': 113, '2851': 113, '486': 107, '2449': 106, '5070': 91, '0389': 90, '496': 89, '99592': 84, '2762': 77})\n",
      "  \n",
      "List of unique icd9 codes from all labels:  ['2859', '99592', '4019', '2724', '25000', '2720', '2851', '2762', '2449', '4280', '0389', '41401', '42731', '51881', '53081', '486', '496', '5070', '5849', '5990']\n"
     ]
    }
   ],
   "source": [
    "#counts by icd9_codes\n",
    "icd9_codes = Counter()\n",
    "for label in discharge_labels:\n",
    "    for icd9_code in label.split():\n",
    "        icd9_codes[icd9_code] += 1\n",
    "print icd9_codes\n",
    "\n",
    "# list of unique icd9_codes and lookups for its index in the vector\n",
    "unique_icd9_codes = list (icd9_codes)\n",
    "index_to_icd9 = dict(enumerate(unique_icd9_codes))\n",
    "icd9_to_id = {v:k for k,v in index_to_icd9.iteritems()}\n",
    "print '  '\n",
    "print 'List of unique icd9 codes from all labels: ', unique_icd9_codes\n",
    "\n",
    "#convert icd9 codes into ids\n",
    "labels_vector= list(map(get_icd9_array,discharge_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "\n",
    "\n",
    "(1) Clean the text data using the same code as the original paper.\n",
    "https://github.com/yoonkim/CNN_sentence\n",
    "\n",
    "(2) Pad each note to the maximum note length, which turns out to be NN. We append special <PAD> tokens to all other notes to make them NN words. Padding sentences to the same length is useful because it allows us to efficiently batch our data since each example in a batch must be of the same length.\n",
    "(3) Build a vocabulary index and map each word to an integer between 0 and 18,765 (the vocabulary size). Each sentence becomes a vector of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def note_preprocessing(data_notes):\n",
    "    notes_stripped = [s.strip() for s in data_notes]\n",
    "    notes_clean = [clean_str(note) for note in notes_stripped ]\n",
    "    notes_canonicalized = [\" \".join (utils.canonicalize_words(note.split(\" \"))) for note in notes_clean ]\n",
    "    \n",
    "    note_words_length =  [len(x.split(\" \")) for x in notes_canonicalized]\n",
    "    max_document_length = max( note_words_length)  \n",
    "    average_length = np.mean(note_words_length)\n",
    "    return max_document_length, average_length, notes_canonicalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " max document length:  7929\n",
      "average document length:  1961.516\n",
      "Vocabulary_size:  23128\n"
     ]
    }
   ],
   "source": [
    "#preprocess documents\n",
    "max_document_length, average_document_length, notes_processed = note_preprocessing(discharge_notes)\n",
    "\n",
    "\n",
    "print ' max document length: ', max_document_length\n",
    "print 'average document length: ', average_document_length\n",
    "\n",
    "#create vocabulary processor\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    \n",
    "# convert words to ids, and each document is padded\n",
    "notes_ids = np.array(list(vocab_processor.fit_transform(notes_processed)))\n",
    "\n",
    "# vocabulary size\n",
    "vocabulary_size = len(vocab_processor.vocabulary_)\n",
    "print 'Vocabulary_size: ', vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"admission date DGDGDGDG DG DG discharge date DGDGDGDG DG DG date of birth DGDGDGDG DG DGDG sex f service medicine history of present illness the patient is a DGDG year old woman with a history of coronary artery disease status post myocardial infarction in DGDGDGDG , status post recent left anterior descending stent with an ejection fraction of DGDG , hypertension , known carotid stenosis , who was admitted to trauma surgical intensive care unit on DG DG , after falling after a blackout and hitting her head the patient had trauma to the head and face the patient had one to two minutes of loss of consciousness the patient denies preceding chest pain , shortness of breath , lightheadedness , dizziness , diaphoresis , visual loss and vertigo she has no history of syncope or loss of consciousness although she had an episode of transient visual loss in the setting of taking sublingual nitroglycerin on last admission the patient had no post ictal confusion in the emergency department , a head ct scan was done which showed an intraparenchymal bleed on the medial portion of the right frontal lobe a ct scan of the spine showed no fracture of subluxation the patient was discharged recently on coumadin on admission , her inr was found to be DG DG on last admission , the patient was evaluated by the neurological service after a transient visual loss the patient was discharged with scheduled follow up with neurology , with results of the mri and mra still pending past medical history DG coronary artery disease status post myocardial infarction in DGDGDGDG on DGDGDGDG DG DG , the patient had a percutaneous transluminal coronary angioplasty stent of a DGDG mid left anterior descending and was started on plavix for nine months and coumadin for a low ejection fraction , and questionable inferior apical aneurysm DG hypertension DG hypercholesterolemia DG history of breast cancer DG history of cerebrovascular accident DG hypothyroidism DG lumbar stenosis DG status post total abdominal hysterectomy DG carotid stenosis DGDG status post appendectomy DGDG history of glaucoma surgery DGDG likely posterior circulation hypoperfusion DGDG chronic renal insufficiency DGDG doppler done in DG DGDGDGDG , showed left common carotid stenosis between DGDG to DGDG and a right subclavian stenosis between DGDG and DGDG medications on admission DG synthroid DGDG micrograms p o q day DG coumadin DG aspirin DGDGDG mg q day DG atenolol DGDG mg twice a day DG elavil DGDG mg q h s DG fioricet p r n DG lipitor DGDG mg q h s DG lisinopril DGDG mg twice a day DG multivitamin DGDG valium DG mg q day DGDG plavix DGDG mg q day allergies the patient has no known drug allergies physical examination on physical examination , the patient was alert and oriented she had swelling over the right eye and forehead the patient was able to follow commands pupils were symmetric and reactive cranial nerves ii through xii intact strength of five out of five throughout sensation intact no pronator drift neck had a cervical collar on lungs were clear to auscultation bilaterally heart was regular rate and rhythm , s1 , s2 abdomen soft , nontender , nondistended extremities with no edema she had ecchymosis and abrasions of her shins bilaterally rectal the patient had normal tone , heme negative back with no tenderness or deformities laboratory on admission , white blood cell count DG DG , hematocrit DGDG DG , platelets DGDGDG sodium DGDGDG , potassium DG DG , chloride DGDGDG , bicarbonate DGDG , bun DGDG , creatinine DG DG , glucose DGDGDG pt DGDG DG , inr DG DG , ptt DGDG DG , ck DGDGDG , mb DG , troponin less than DG DG electrocardiogram unchanged from prior hospital course the patient was admitted to trauma surgical intensive care unit for observation coumadin , aspirin and plavix were held the patient was felt stable and was transferred to medicine in terms of the syncopal work up , it was unclear whether this was related to cardiac versus neurologic cardiology was consulted and recommended an electrophysiology study additionally , the patient was ruled out for a myocardial infarction she had an electrophysiology study performed which showed normal sinus , no sinus dysfunction , normal per kg conduction , no inducible ventricular tachycardia the patient was followed by neurosurgery and neurology it was felt that her intracranial bleed was small the patient initially had aspirin , plavix and coumadin withheld then she was started on aspirin since her intracranial bleed seemed small and the patient had no neurological deficits it was agreed upon between neurology and cardiology that she could be restarted back on plavix , however , it is felt that restarting coumadin is too risky at this time during the hospital course , the patient had episodes of chest pain there were no year \\\\( DG digits \\\\) changes the patient was ruled out for a myocardial infarction it was felt that the patient 's syncopal episode was not due to neurovascular causes , however , the mri mra revealed a left cca stenosis and no left vertebral artery was seen furthermore , a left subclavian stenosis was also noted due to these findings , it was felt that the patient should maintain a blood pressure of greater than DGDGDG to maintain adequate perfusion to her brain the syncopal event is most likely from vasovagal or orthostatic hypotension , however , if it recurs , further evaluation is warranted the patient was noted to have a mass in the right upper lung on ct scan , however , this was seen on prior ct scans and the patient says it is related to radiation therapy from her radiation therapy when she had breast cancer it was felt that further evaluation of this mass would not be followed up as an inpatient and will be deferred to outpatient management condition on discharge stable discharge status to home discharge diagnoses DG syncope , most like vasovagal versus orthostatic hypotension DG small intracranial hemorrhage DG electrophysiology study with no evidence of inducible ventricular tachycardia , sinus dysfunction or conduction abnormalities DG coronary artery disease , ruled out for myocardial infarction DG carotid disease DG hypertension discharge instructions DG the patient should follow up with her cardiologist DG the patient should follow up with her neurologist DG the patient should also follow up with her primary care physician as scheduled as before discharge medications DG atenolol DGDG mg twice a day DG elavil DGDG mg q h s DG lisinopril DGDG mg twice a day DG lipitor DGDG mg q day DG plavix DGDG mg q day DG aspirin DGDGDG mg q day DG levoxyl DGDG micrograms q day first name11 \\\\( name pattern1 \\\\) last name \\\\( namepattern1 \\\\) DGDGDGDG , m d md number \\\\( DG \\\\) DGDGDGDG dictated by name8 \\\\( md \\\\) DGDGDGDGDGDG medquist36 d DGDGDGDG DG DGDG DGDG DGDG t DGDGDGDG DG DGDG DGDG DGDG job job number DGDGDGDGDGDG\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes_processed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question?\n",
    "VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV   \n",
    "what do we do if the test data has a document with a bigger length than the max for the padding? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transforming to embeddings using word2vec\n",
    "\n",
    "From: \"A Comparison of Rule-Based and Deep Learning Models for Patient Phenotyping\"\n",
    "\n",
    "\"We pre-train our embeddings with word2vec on all discharge notes available in the MIMIC-III database.   \n",
    "The word embeddings of all words in the text to classify are concatenated and used as input to the\n",
    "convolutional layer. Convolutions detect a signal from a combination of adjacent inputs. We\n",
    "combine multiple convolutions of different lengths to evaluate phrases that are anywhere from\n",
    "two to five words long,\"   \n",
    "\n",
    "(tf-idf is removing negations..  embedding is taking care of mispellings.. we may need further training-tuning because of medical terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://code.google.com/archive/p/word2vec/\n",
    "    \n",
    "Pre-trained word and phrase vectors\n",
    "\n",
    "\"We are publishing pre-trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in [2]. The archive is available here: GoogleNews-vectors-negative300.bin.gz.\"   \n",
    "\n",
    "### for now we wil train our own embeddings, but word2vec will be better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set samples: 700\n",
      "Dev set samples: 150\n",
      "Test set samples: 150\n"
     ]
    }
   ],
   "source": [
    "def split_file(data, train_frac = 0.7, dev_frac = 0.15):   \n",
    "    train_split_idx = int(train_frac * len(data))\n",
    "    dev_split_idx = int ((train_frac + dev_frac)* len(data))\n",
    "    train_data = data[:train_split_idx]\n",
    "    dev_data = data[train_split_idx:dev_split_idx]\n",
    "    test_data = data[dev_split_idx:]\n",
    "    return train_data, dev_data, test_data\n",
    "\n",
    "\n",
    "train_notes, dev_notes, test_notes = split_file (notes_ids)\n",
    "train_labels, dev_labels, test_labels = split_file (labels_vector)\n",
    "print 'Training set samples:', len (train_notes)\n",
    "print 'Dev set samples:', len (dev_notes)\n",
    "print 'Test set samples:', len (test_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Training\n",
    "\n",
    "here is an example of a CNN to classify text.. our model will have different values for d (embedding-size, region sizes, etc)\n",
    "<img src=\"CNN_for_text2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the CNN used with the MIMIC discharge summaries\n",
    "<img src=\"mimic_CNN_text_classification.png\"/>\n",
    "\n",
    "\n",
    "\"For the CNN model, we used 100 filters for each of the widths 2, 3, 4, and 5.    \n",
    "To prevent overfitting, we set the dropout probability to 0.5 and used L2-normalization to normalize word\n",
    "embeddings to have a max norm of 3.64     \n",
    "The model was trained using adadelta with an initial learning rate of 1 for 20 epochs.   \n",
    "The CNN model was implemented using Lua and the Torch7 framework.66    \n",
    "All baseline models were implemented using Python with the scikit-learn library.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### sources:\n",
    "http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/  \n",
    "http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/   \n",
    "https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py   \n",
    "https://www.tensorflow.org/get_started/mnist/pros   \n",
    "https://www.tensorflow.org/api_docs/python/tf/nn/conv2d   \n",
    " \n",
    " multi-label\n",
    " https://github.com/may-/cnn-re-tf/blob/master/cnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: \"A Comparison of Rule-Based and Deep Learning Models for Patient Phenotyping\"\n",
    "\n",
    "\"For the CNN model, we used 100 filters for each of the widths 2, 3, 4, and 5.   \n",
    "To prevent overfitting, we set the dropout probability to 0.5 and used L2-normalization to normalize word\n",
    "embeddings to have a max norm of 3.64   \n",
    "The model was trained using adadelta with an initial learning rate of 1 for 20 epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, X, y, batch_size):\n",
    "    for batch in xrange(0, X.shape[0], batch_size):\n",
    "        # x SHAPE:   [batch_size, sequence_length, embedding_size]\n",
    "        X_batch = X[batch : batch + batch_size]\n",
    "        y_batch = y[batch : batch + batch_size]\n",
    "        feed_dict = {lm.input_x:X_batch,lm.input_y:y_batch,lm.dropout_keep_prob:0.5}\n",
    "        #loss, train_op_value =  session.run( [lm.loss,lm.train],feed_dict=feed_dict ) \n",
    "        loss, _, step = session.run([lm.loss, lm.train_op, lm.global_step], feed_dict)\n",
    "        print 'batch: %d, loss: %5.5f' % (batch, loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_icd9_codes(lm, session, x_data, y_data):\n",
    "    total_y_hat = []\n",
    "    for batch in xrange(0, x_data.shape[0], batch_size):\n",
    "        X_batch = x_data[batch : batch + batch_size]\n",
    "        Y_batch = y_data[batch : batch + batch_size]\n",
    "        y_hat_out = session.run(lm.y_hat, feed_dict={lm.input_x:X_batch,lm.input_y:Y_batch, lm.dropout_keep_prob: 1.0})\n",
    "        total_y_hat.extend(y_hat_out)\n",
    "    return  total_y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build tensorflow graphs\n",
    "reload(cnn_model)\n",
    "\n",
    "# Model parameters\n",
    "\n",
    "model_params = dict(vocab_size= vocabulary_size, sequence_length=max_document_length, learning_rate=1.0,\\\n",
    "                    embedding_size=128, num_classes=20, filter_sizes=[2,3,4,5], num_filters=100)\n",
    "\n",
    "# Build and Train Model\n",
    "cnn = cnn_model.NNLM(**model_params)\n",
    "cnn.BuildCoreGraph()\n",
    "cnn.BuildTrainGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TF_SAVEDIR = \"tf_saved\"\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"cnn_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_num: 0\n",
      "batch: 0, loss: 41.75618\n",
      "batch: 50, loss: 43.49678\n",
      "batch: 100, loss: 44.85231\n",
      "batch: 150, loss: 47.05762\n",
      "batch: 200, loss: 41.92564\n",
      "batch: 250, loss: 38.48314\n",
      "batch: 300, loss: 33.19728\n",
      "batch: 350, loss: 37.97263\n",
      "batch: 400, loss: 38.17122\n",
      "batch: 450, loss: 37.48780\n",
      "batch: 500, loss: 44.62095\n",
      "batch: 550, loss: 43.39772\n",
      "batch: 600, loss: 36.83818\n",
      "batch: 650, loss: 35.93096\n",
      "epoch_num: 1\n",
      "batch: 0, loss: 32.72146\n",
      "batch: 50, loss: 35.67733\n",
      "batch: 100, loss: 39.73853\n",
      "batch: 150, loss: 43.30456\n",
      "batch: 200, loss: 34.22692\n",
      "batch: 250, loss: 36.67789\n",
      "batch: 300, loss: 30.86436\n",
      "batch: 350, loss: 32.74348\n",
      "batch: 400, loss: 35.99274\n",
      "batch: 450, loss: 39.22559\n",
      "batch: 500, loss: 41.25626\n",
      "batch: 550, loss: 46.58323\n",
      "batch: 600, loss: 33.44085\n",
      "batch: 650, loss: 33.82124\n",
      "epoch_num: 2\n",
      "batch: 0, loss: 34.25573\n",
      "batch: 50, loss: 37.50615\n",
      "batch: 100, loss: 42.85917\n",
      "batch: 150, loss: 47.14633\n",
      "batch: 200, loss: 36.98771\n",
      "batch: 250, loss: 39.02951\n",
      "batch: 300, loss: 35.77827\n",
      "batch: 350, loss: 40.53747\n",
      "batch: 400, loss: 41.71246\n",
      "batch: 450, loss: 39.19783\n",
      "batch: 500, loss: 43.06400\n",
      "batch: 550, loss: 52.02052\n",
      "batch: 600, loss: 43.11234\n",
      "batch: 650, loss: 41.10829\n",
      "epoch_num: 3\n",
      "batch: 0, loss: 39.00880\n",
      "batch: 50, loss: 45.30611\n",
      "batch: 100, loss: 47.50816\n",
      "batch: 150, loss: 46.79923\n",
      "batch: 200, loss: 42.50680\n",
      "batch: 250, loss: 41.47519\n",
      "batch: 300, loss: 39.86002\n",
      "batch: 350, loss: 41.94531\n",
      "batch: 400, loss: 51.17062\n",
      "batch: 450, loss: 48.78630\n",
      "batch: 500, loss: 52.25023\n",
      "batch: 550, loss: 59.69294\n",
      "batch: 600, loss: 51.28868\n",
      "batch: 650, loss: 53.15862\n",
      "epoch_num: 4\n",
      "batch: 0, loss: 45.75160\n",
      "batch: 50, loss: 52.44857\n",
      "batch: 100, loss: 65.22059\n",
      "batch: 150, loss: 68.86284\n",
      "batch: 200, loss: 54.40019\n",
      "batch: 250, loss: 57.42575\n",
      "batch: 300, loss: 44.42976\n",
      "batch: 350, loss: 64.19060\n",
      "batch: 400, loss: 69.62742\n",
      "batch: 450, loss: 62.70707\n",
      "batch: 500, loss: 64.78638\n",
      "batch: 550, loss: 79.98526\n",
      "batch: 600, loss: 67.39520\n",
      "batch: 650, loss: 67.99669\n",
      "predicting training now \n",
      "predicting dev set now\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "with cnn.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=cnn.graph) as session:\n",
    "    session.run(initializer)\n",
    "    #training\n",
    "    for epoch_num in xrange(num_epochs):\n",
    "        print 'epoch_num:' , epoch_num\n",
    "        run_epoch(cnn, session, train_notes, train_labels, batch_size)\n",
    "    saver.save(session, trained_filename)\n",
    "    print 'predicting training now '\n",
    "    train_y_hat = predict_icd9_codes(cnn, session, train_notes, train_labels)   \n",
    "    print 'predicting dev set now'\n",
    "    dev_y_hat = predict_icd9_codes(cnn, session, dev_notes, dev_labels)\n",
    "    print 'done!'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[  1.12788871e-06   1.11937751e-11   2.94459522e-01   1.68707527e-06\n",
      "   4.85506680e-05   2.66462870e-08   2.13917624e-03   2.69456701e-09\n",
      "   3.77299926e-07   6.53665960e-01   5.19855867e-14   1.84163284e-02\n",
      "   2.39666998e-02   2.96574039e-03   1.37168263e-05   6.22122070e-06\n",
      "   2.89892884e-11   8.31788967e-08   4.31381073e-03   9.51213337e-07]\n",
      "0.999999981916\n"
     ]
    }
   ],
   "source": [
    "print train_labels[0]\n",
    "print train_y_hat[0]\n",
    "print sum (train_y_hat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ranking loss:  0.315731861733\n",
      "Development ranking loss:  0.330642802286\n"
     ]
    }
   ],
   "source": [
    "# ranking loss\n",
    "training_ranking_loss = label_ranking_loss(train_labels, train_y_hat)\n",
    "print \"Training ranking loss: \", training_ranking_loss\n",
    "dev_ranking_loss = label_ranking_loss(dev_labels, dev_y_hat)\n",
    "print \"Development ranking loss: \", dev_ranking_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO  create a model for thresholding\n",
    "\n",
    "Large-scale Multi-label Text Classificationâ€”Revisiting Neural Networks\n",
    "\n",
    "\n",
    "\"3.3 Thresholding\n",
    "Once training of the neural network is finished, its output may be interpreted as a probability\n",
    "distribution p (ojx) over the labels for a given document x. The probability distribution\n",
    "can be used to rank labels, but additional measures are needed in order to split\n",
    "the ranking into relevant and irrelevant labels. For transforming the ranked list of labels\n",
    "into a set of binary predictions, we train a multi-label threshold predictor from training\n",
    "data. This sort of thresholding methods are also used in [6, 31]\n",
    "For each document xm, labels are sorted by the probabilities in decreasing order.\n",
    "Ideally, if NNs successfully learn a mapping function f , all correct (positive) labels\n",
    "will be placed on top of the sorted list and there should be large margin between the set\n",
    "of positive labels and the set of negative labels. Using F1 score as a reference measure,\n",
    "we calculate classification performances at every pair of successive positive labels and\n",
    "choose a threshold value tm that produces the best performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_f1_score(y_true,y_hat,threshold, average):\n",
    "    hot_y = np.where(np.array(y_hat) > threshold, 1, 0)\n",
    "    return f1_score(np.array(y_true), hot_y, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores\n",
      "threshold | training | dev  \n",
      "0.005:      0.350      0.340\n",
      "0.010:      0.339      0.330\n",
      "0.020:      0.321      0.299\n",
      "0.030:      0.306      0.291\n",
      "0.040:      0.301      0.285\n",
      "0.050:      0.292      0.285\n",
      "0.060:      0.287      0.280\n",
      "0.100:      0.276      0.264\n",
      "0.500:      0.200      0.192\n"
     ]
    }
   ],
   "source": [
    "print 'F1 scores'\n",
    "print 'threshold | training | dev  '\n",
    "f1_score_average = 'micro'\n",
    "for threshold in [ 0.005, 0.01,0.02,0.03,0.04,0.05,0.06, 0.1, 0.5]:\n",
    "    train_f1 = get_f1_score(train_labels, train_y_hat,threshold,f1_score_average)\n",
    "    dev_f1 = get_f1_score(dev_labels, dev_y_hat,threshold,f1_score_average)\n",
    "    print '%1.3f:      %1.3f      %1.3f' % (threshold,train_f1, dev_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Thoughts so far\n",
    "\n",
    "The CNN loss is stuck, the model is not learning much. The F1 score of 35%,which is close to the baseline model that always predict the top 4 most common icd-9 code and to the NN Baseline.\n",
    "\n",
    "There is a LSTM model by this paper: \"Applying Deep Learning to ICD-9 Multi-label Classification from Medical Records\" which did achieve a 42% F1-score. (https://cs224d.stanford.edu/reports/priyanka.pdf)\n",
    "\n",
    "(note:  I think the CNN should be getting about 40% also)\n",
    "\n",
    "The \"A Comparison of Rule-Based and Deep Learning Models for Patient Phenotyping\"  study did get a 70% F1-score, but they don't use the icd9-labels but phenotypes labels they annotated themselved (via a group of medical professionals). (https://arxiv.org/abs/1703.08705). There were ONLY 10 phenotypes.\n",
    "\n",
    "The discharge summaries are labeled with ICD9-codes that are leaves in the ICD9-hierarchy (which has hundreds of ICD9-codes), then maybe these leave nodes are too specific and difficult to predict, one experiment would be to replaced all the ICD9-codes with their parent in the second or third level in the hierarchy and see if predictions work better that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
