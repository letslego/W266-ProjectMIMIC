{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import sys \n",
    "\n",
    "#keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "# Custom functions\n",
    "sys.path.append(\"../pipeline\")\n",
    "import icd9_cnn_model\n",
    "import database_selection\n",
    "import vectorization\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/disch_notes_all_icd9.csv',\n",
    "                 names = ['HADM_ID', 'SUBJECT_ID', 'DATE', 'ICD9','TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_TOP = 20 \n",
    "full_df, top_codes = database_selection.filter_top_codes(df, 'ICD9', N_TOP, filter_empty = True)\n",
    "#df = full_df.head(1000)\n",
    "df = full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preprocess icd9 codes\n",
    "labels = vectorization.vectorize_icd_column(df, 'ICD9', top_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 130488\n",
      "Average note length: 1728.09244863\n",
      "Max note length: 10924\n",
      "Final Vocabulary: 130488\n",
      "Final Max Sequence Length: 5000\n"
     ]
    }
   ],
   "source": [
    "#preprocess notes\n",
    "MAX_VOCAB = None # to limit original number of words (None if no limit)\n",
    "MAX_SEQ_LENGTH = 5000 # to limit length of word sequence (None if no limit)\n",
    "df.TEXT = vectorization.clean_notes(df, 'TEXT')\n",
    "data, dictionary, MAX_VOCAB = vectorization.vectorize_notes(df.TEXT, MAX_VOCAB, verbose = True)\n",
    "data, MAX_SEQ_LENGTH = vectorization.pad_notes(data, MAX_SEQ_LENGTH)\n",
    "print(\"Final Vocabulary: %s\" % MAX_VOCAB)\n",
    "print(\"Final Max Sequence Length: %s\" % MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train: ', (30794, 5000), (30794, 20))\n",
      "('Validation: ', (8798, 5000), (8798, 20))\n",
      "('Test: ', (4400, 5000), (4400, 20))\n"
     ]
    }
   ],
   "source": [
    "#split sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = helpers.train_val_test_split(\n",
    "    data, labels, val_size=0.2, test_size=0.1, random_state=101)\n",
    "print(\"Train: \", X_train.shape, y_train.shape)\n",
    "print(\"Validation: \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete temporary variables to free some memory\n",
    "del df, data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Vocabulary in notes:', 130488)\n",
      "('Vocabulary in original embedding:', 21056)\n",
      "('Vocabulary intersection:', 20620)\n"
     ]
    }
   ],
   "source": [
    "#creating embeddings\n",
    "#EMBEDDING_LOC = '../data/glove.6B.100d.txt' # location of embedding\n",
    "# embedding pre-trained will all MIMIC notes\n",
    "EMBEDDING_LOC = '../data/notes.100.txt' # location of embedding\n",
    "EMBEDDING_DIM = 100 # given the glove that we chose\n",
    "embedding_matrix, embedding_dict = vectorization.embedding_matrix(EMBEDDING_LOC,\n",
    "                                                                  dictionary, EMBEDDING_DIM, verbose = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN for text classification\n",
    "\n",
    "Based on the following papers and links:\n",
    "* \"Convolutional Neural Networks for Sentence Classification\"   \n",
    "* \"A Sensitivity Analysis of (and Practitionersï¿½ Guide to) Convolutional Neural Networks for Sentence Classification\"\n",
    "* http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "* https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras/blob/master/sentiment_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 5000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 5000, 100)     13048900    input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)                (None, 4999, 100)     20100       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)                (None, 4998, 100)     30100       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)                (None, 4997, 100)     40100       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)                (None, 4996, 100)     50100       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)   (None, 1, 100)        0           conv1d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)   (None, 1, 100)        0           conv1d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)   (None, 1, 100)        0           conv1d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)   (None, 1, 100)        0           conv1d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 100)           0           max_pooling1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 100)           0           max_pooling1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 100)           0           max_pooling1d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 100)           0           max_pooling1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 400)           0           flatten_1[0][0]                  \n",
      "                                                                   flatten_2[0][0]                  \n",
      "                                                                   flatten_3[0][0]                  \n",
      "                                                                   flatten_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 400)           0           concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 20)            8020        dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 13,197,320\n",
      "Trainable params: 13,197,320\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "reload(icd9_cnn_model)\n",
    "#### build model\n",
    "model = icd9_cnn_model.build_icd9_cnn_model (input_seq_length=MAX_SEQ_LENGTH, max_vocab = MAX_VOCAB,\n",
    "                             external_embeddings = True,\n",
    "                             embedding_dim=EMBEDDING_DIM,embedding_matrix=embedding_matrix,\n",
    "                             num_filters = 100, filter_sizes=[2,3,4,5],\n",
    "                             training_dropout_keep_prob=0.5,\n",
    "                             num_classes=N_TOP )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30794 samples, validate on 8798 samples\n",
      "Epoch 1/5\n",
      "1008s - loss: 0.4447 - acc: 0.8289 - val_loss: 0.3207 - val_acc: 0.8677\n",
      "Epoch 2/5\n",
      "984s - loss: 0.3245 - acc: 0.8698 - val_loss: 0.2738 - val_acc: 0.8868\n",
      "Epoch 3/5\n",
      "981s - loss: 0.2889 - acc: 0.8835 - val_loss: 0.2522 - val_acc: 0.8978\n",
      "Epoch 4/5\n",
      "980s - loss: 0.2708 - acc: 0.8915 - val_loss: 0.2422 - val_acc: 0.9047\n",
      "Epoch 5/5\n",
      "977s - loss: 0.2605 - acc: 0.8965 - val_loss: 0.2391 - val_acc: 0.9050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f87e163d310>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first 5 epochs\n",
    "model.fit(X_train, y_train, batch_size=50, epochs=5, validation_data=(X_val, y_val), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('models/cnn_5_epochs_allr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores\n",
      "threshold | training | dev  \n",
      "0.020:      0.358      0.353\n",
      "0.030:      0.407      0.400\n",
      "0.040:      0.456      0.446\n",
      "0.050:      0.502      0.488\n",
      "0.055:      0.523      0.508\n",
      "0.058:      0.534      0.518\n",
      "0.060:      0.541      0.525\n",
      "0.080:      0.602      0.582\n",
      "0.100:      0.645      0.621\n",
      "0.200:      0.732      0.704\n",
      "0.300:      0.747      0.717\n",
      "0.400:      0.738      0.707\n",
      "0.500:      0.712      0.679\n",
      "0.600:      0.668      0.631\n",
      "0.700:      0.594      0.558\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.predict(X_train, batch_size=200)\n",
    "pred_dev = model.predict(X_val, batch_size=200)\n",
    "helpers.show_f1_score(y_train, pred_train, y_val, pred_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30794 samples, validate on 8798 samples\n",
      "Epoch 1/2\n",
      "834s - loss: 0.2518 - acc: 0.9010 - val_loss: 0.2371 - val_acc: 0.9076\n",
      "Epoch 2/2\n",
      "837s - loss: 0.2437 - acc: 0.9041 - val_loss: 0.2367 - val_acc: 0.9075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f87a57c4310>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 more epochs\n",
    "model.fit(X_train, y_train, batch_size=50, epochs=2, validation_data=(X_val, y_val), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('models/cnn_7_epochs_allr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores\n",
      "threshold | training | dev  \n",
      "0.020:      0.373      0.366\n",
      "0.030:      0.424      0.412\n",
      "0.040:      0.472      0.455\n",
      "0.050:      0.515      0.494\n",
      "0.055:      0.536      0.512\n",
      "0.058:      0.548      0.522\n",
      "0.060:      0.555      0.528\n",
      "0.080:      0.622      0.587\n",
      "0.100:      0.669      0.629\n",
      "0.200:      0.759      0.713\n",
      "0.300:      0.774      0.724\n",
      "0.400:      0.767      0.714\n",
      "0.500:      0.746      0.691\n",
      "0.600:      0.708      0.651\n",
      "0.700:      0.644      0.584\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.predict(X_train, batch_size=200)\n",
    "pred_dev = model.predict(X_val, batch_size=200)\n",
    "helpers.show_f1_score(y_train, pred_train, y_val, pred_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## early termination at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30794 samples, validate on 8798 samples\n",
      "Epoch 1/2\n",
      "949s - loss: 0.2371 - acc: 0.9068 - val_loss: 0.2388 - val_acc: 0.9073\n",
      "Epoch 2/2\n",
      "942s - loss: 0.2281 - acc: 0.9105 - val_loss: 0.2427 - val_acc: 0.9044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f87a5d1e610>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=50, epochs=2, validation_data=(X_val, y_val), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('models/cnn_8_epochs_allr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores\n",
      "threshold | training | dev  \n",
      "0.020:      0.406      0.393\n",
      "0.030:      0.470      0.447\n",
      "0.040:      0.526      0.494\n",
      "0.050:      0.574      0.534\n",
      "0.055:      0.596      0.551\n",
      "0.058:      0.607      0.561\n",
      "0.060:      0.614      0.567\n",
      "0.080:      0.674      0.617\n",
      "0.100:      0.714      0.650\n",
      "0.200:      0.793      0.712\n",
      "0.300:      0.804      0.716\n",
      "0.400:      0.792      0.700\n",
      "0.500:      0.761      0.670\n",
      "0.600:      0.713      0.627\n",
      "0.700:      0.642      0.564\n"
     ]
    }
   ],
   "source": [
    "pred_train = model.predict(X_train, batch_size=200)\n",
    "pred_dev = model.predict(X_val, batch_size=200)\n",
    "helpers.show_f1_score(y_train, pred_train, y_val, pred_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## previous run with dropout  =0.9\n",
    "\n",
    "too much, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30794 samples, validate on 8798 samples\n",
      "Epoch 1/10\n",
      "921s - loss: 0.6316 - acc: 0.7930 - val_loss: 0.4130 - val_acc: 0.8373\n",
      "Epoch 2/10\n",
      "921s - loss: 0.4279 - acc: 0.8351 - val_loss: 0.3831 - val_acc: 0.8482\n",
      "Epoch 3/10\n",
      "919s - loss: 0.4002 - acc: 0.8457 - val_loss: 0.3557 - val_acc: 0.8612\n",
      "Epoch 4/10\n",
      "913s - loss: 0.3839 - acc: 0.8532 - val_loss: 0.3416 - val_acc: 0.8663\n",
      "Epoch 5/10\n",
      "906s - loss: 0.3739 - acc: 0.8571 - val_loss: 0.3316 - val_acc: 0.8677\n",
      "Epoch 6/10\n",
      "909s - loss: 0.3677 - acc: 0.8595 - val_loss: 0.3270 - val_acc: 0.8684\n",
      "Epoch 7/10\n",
      "913s - loss: 0.3621 - acc: 0.8616 - val_loss: 0.3195 - val_acc: 0.8727\n",
      "Epoch 8/10\n",
      "909s - loss: 0.3571 - acc: 0.8635 - val_loss: 0.3174 - val_acc: 0.8727\n",
      "Epoch 9/10\n",
      "906s - loss: 0.3525 - acc: 0.8654 - val_loss: 0.3120 - val_acc: 0.8747\n",
      "Epoch 10/10\n",
      "908s - loss: 0.3502 - acc: 0.8660 - val_loss: 0.3148 - val_acc: 0.8727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe7ddd6f910>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=50, epochs=10, validation_data=(X_val, y_val), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train, batch_size=200)\n",
    "pred_dev = model.predict(X_val, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores\n",
      "threshold | training | dev  \n",
      "0.020:      0.303      0.301\n",
      "0.030:      0.313      0.311\n",
      "0.040:      0.327      0.323\n",
      "0.050:      0.342      0.338\n",
      "0.055:      0.351      0.346\n",
      "0.058:      0.357      0.351\n",
      "0.060:      0.360      0.354\n",
      "0.080:      0.403      0.394\n",
      "0.100:      0.448      0.437\n",
      "0.200:      0.610      0.589\n",
      "0.300:      0.611      0.591\n",
      "0.400:      0.559      0.538\n",
      "0.500:      0.482      0.462\n",
      "0.600:      0.393      0.376\n",
      "0.700:      0.319      0.306\n"
     ]
    }
   ],
   "source": [
    "helpers.show_f1_score(y_train, pred_train, y_val, pred_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('models/cnn_10_epochs_allr.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results with external embeddings = True , no additional training,  top 20\n",
    "```\n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.020:      0.337      0.329\n",
    "0.030:      0.360      0.353\n",
    "0.040:      0.365      0.374\n",
    "0.050:      0.372      0.375\n",
    "0.055:      0.370      0.377\n",
    "0.058:      0.369      0.375\n",
    "0.060:      0.368      0.375\n",
    "0.080:      0.348      0.361\n",
    "0.100:      0.309      0.319\n",
    "0.200:      0.198      0.208\n",
    "0.300:      0.157      0.138\n",
    "0.500:      0.000      0.000\n",
    "```\n",
    "\n",
    "### Results with external embeddings = False, top 20\n",
    "```\n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.020:      0.288      0.300\n",
    "0.030:      0.327      0.322\n",
    "0.040:      0.371      0.363\n",
    "0.050:      0.380      0.391\n",
    "0.055:      0.412      0.383\n",
    "0.058:      0.403      0.394\n",
    "0.060:      0.394      0.389\n",
    "0.080:      0.385      0.390\n",
    "0.100:      0.229      0.225\n",
    "0.200:      0.000      0.000\n",
    "0.300:      0.000      0.000\n",
    "0.500:      0.000      0.000\n",
    "```\n",
    "\n",
    "### Results with external embedding and training them , top 20\n",
    "```\n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.020:      0.334      0.333\n",
    "0.030:      0.362      0.360\n",
    "0.040:      0.366      0.374\n",
    "0.050:      0.373      0.380\n",
    "0.055:      0.374      0.382\n",
    "0.058:      0.376      0.376\n",
    "0.060:      0.376      0.378\n",
    "0.080:      0.387      0.371\n",
    "0.100:      0.366      0.350\n",
    "0.200:      0.179      0.171\n",
    "0.300:      0.020      0.020\n",
    "0.500:      0.000      0.000\n",
    "\n",
    "```\n",
    "\n",
    "### Results with external Embeddings = False, top 10, \n",
    "We can compare this setup with the LSTM published in the paper \"Applying Deep Learning to ICD-9 Multi-label Classification from Medical Records\", they got a F1-score of about 0.4168, we are getting 0.447\n",
    "\n",
    "``` \n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.020:      0.399      0.407\n",
    "0.030:      0.399      0.407\n",
    "0.040:      0.399      0.407\n",
    "0.050:      0.408      0.413\n",
    "0.055:      0.433      0.420\n",
    "0.058:      0.437      0.430\n",
    "0.060:      0.432      0.427\n",
    "0.080:      0.501      0.463\n",
    "0.100:      0.446      0.463\n",
    "0.200:      0.206      0.066\n",
    "0.300:      0.000      0.000\n",
    "0.500:      0.000      0.000\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Notes:\n",
    "\n",
    "\n",
    "(1) There is a LSTM model by this paper: \"Applying Deep Learning to ICD-9 Multi-label Classification from Medical Records\" which did achieve a 42% F1-score. (https://cs224d.stanford.edu/reports/priyanka.pdf), but it only uses the top 10 icd9 codes. We are getting 46% (just running with 1000 notes so far)\n",
    "\n",
    "\n",
    "(2) The \"A Comparison of Rule-Based and Deep Learning Models for Patient Phenotyping\"  study did get a 70% F1-score, but they don't use the icd9-labels but phenotypes labels they annotated themselved (via a group of medical professionals). (https://arxiv.org/abs/1703.08705). There were ONLY 10 phenotypes.\n",
    "\n",
    "The discharge summaries are labeled with ICD9-codes that are leaves in the ICD9-hierarchy (which has hundreds of ICD9-codes), then maybe these leave nodes are too specific and difficult to predict, one experiment would be to replaced all the ICD9-codes with their parent in the second or third level in the hierarchy and see if predictions work better that way.   \n",
    "\n",
    "(3) our baseline with top 20 codes had a f1-score of 35% (assigning top 4 icd9 codes to all notes, using a CNN with no external embeddings is getting about 40% f1-score.. a little better than the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
