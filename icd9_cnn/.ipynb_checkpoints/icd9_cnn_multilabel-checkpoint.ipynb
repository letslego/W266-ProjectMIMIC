{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import *\n",
    "import re\n",
    "from tensorflow.contrib import learn\n",
    "import sys, os\n",
    "import tensorflow as tf\n",
    "import cnn_model\n",
    "import utils\n",
    "\n",
    "from sklearn.metrics import label_ranking_loss\n",
    "from sklearn.metrics import f1_score\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Sources:\n",
    "http://ruder.io/deep-learning-nlp-best-practices/index.html#classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the dataset:  45837\n"
     ]
    }
   ],
   "source": [
    "#with open('../../../psql_files/disch_notes_all_icd9.csv', 'rb') as csvfile:\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "with open('../baseline/psql_files/dis_notes_icd9.csv', 'rb') as csvfile:\n",
    "    discharge_notes_reader = csv.reader(csvfile)\n",
    "    discharge_notes_list = list(discharge_notes_reader)    \n",
    "random.shuffle(discharge_notes_list)\n",
    "\n",
    "print \"Number of records in the dataset: \", len (discharge_notes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will take only 10,000 records to compare with NN baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting for 1,000 just for programming\n",
    "number_records = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of discharge clinical notes:  10000\n"
     ]
    }
   ],
   "source": [
    "discharge_notes_icd9 = np.asarray(discharge_notes_list[0:number_records])\n",
    "print 'Number of discharge clinical notes: ', len(discharge_notes_icd9)\n",
    "discharge_notes= discharge_notes_icd9[:,3]\n",
    "discharge_labels = discharge_notes_icd9[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats about Notes  (TODO:)\n",
    "* vocabulary of size\n",
    "* find out notes that are too large, outliers to take out (otherwise the embeddings will pad a lot of zeroes to the other note-vectors("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting icd9 labels to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transforming list of icd_codes into a vector\n",
    "def get_icd9_array(icd9_codes):\n",
    "    icd9_index_array = [0]*len(unique_icd9_codes)\n",
    "    for icd9_code in icd9_codes.split():\n",
    "        index = icd9_to_id [icd9_code]\n",
    "        icd9_index_array[index] = 1\n",
    "    return icd9_index_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'4019': 4500, '4280': 2916, '42731': 2881, '41401': 2781, '25000': 2001, '5849': 1994, '2724': 1834, '51881': 1617, '5990': 1472, '53081': 1379, '2720': 1289, '2859': 1200, '486': 1080, '2449': 1051, '2851': 1005, '496': 984, '2762': 964, '5070': 849, '99592': 836, '0389': 817})\n",
      "  \n",
      "List of unique icd9 codes from all labels:  ['2859', '99592', '4280', '2724', '25000', '2720', '2851', '2762', '2449', '4019', '0389', '41401', '53081', '5990', '42731', '486', '5070', '496', '51881', '5849']\n"
     ]
    }
   ],
   "source": [
    "#counts by icd9_codes\n",
    "icd9_codes = Counter()\n",
    "for label in discharge_labels:\n",
    "    for icd9_code in label.split():\n",
    "        icd9_codes[icd9_code] += 1\n",
    "print icd9_codes\n",
    "\n",
    "# list of unique icd9_codes and lookups for its index in the vector\n",
    "unique_icd9_codes = list (icd9_codes)\n",
    "index_to_icd9 = dict(enumerate(unique_icd9_codes))\n",
    "icd9_to_id = {v:k for k,v in index_to_icd9.iteritems()}\n",
    "print '  '\n",
    "print 'List of unique icd9 codes from all labels: ', unique_icd9_codes\n",
    "\n",
    "#convert icd9 codes into ids\n",
    "labels_vector= list(map(get_icd9_array,discharge_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "\n",
    "\n",
    "(1) Clean the text data using the same code as the original paper.\n",
    "https://github.com/yoonkim/CNN_sentence\n",
    "\n",
    "(2) Pad each note to the maximum note length, which turns out to be NN. We append special <PAD> tokens to all other notes to make them NN words. Padding sentences to the same length is useful because it allows us to efficiently batch our data since each example in a batch must be of the same length.\n",
    "(3) Build a vocabulary index and map each word to an integer between 0 and 18,765 (the vocabulary size). Each sentence becomes a vector of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def note_preprocessing(data_notes):\n",
    "    notes_stripped = [s.strip() for s in data_notes]\n",
    "    notes_clean = [clean_str(note) for note in notes_stripped ]\n",
    "    notes_canonicalized = [\" \".join (utils.canonicalize_words(note.split(\" \"))) for note in notes_clean ]\n",
    "    \n",
    "    note_words_length =  [len(x.split(\" \")) for x in notes_canonicalized]\n",
    "    max_document_length = max( note_words_length)  \n",
    "    average_length = np.mean(note_words_length)\n",
    "    return max_document_length, average_length, notes_canonicalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " max document length:  9554\n",
      "average document length:  1931.5146\n",
      "Vocabulary_size:  65928\n"
     ]
    }
   ],
   "source": [
    "#preprocess documents\n",
    "max_document_length, average_document_length, notes_processed = note_preprocessing(discharge_notes)\n",
    "\n",
    "\n",
    "print ' max document length: ', max_document_length\n",
    "print 'average document length: ', average_document_length\n",
    "\n",
    "#create vocabulary processor\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    \n",
    "# convert words to ids, and each document is padded\n",
    "notes_ids = np.array(list(vocab_processor.fit_transform(notes_processed)))\n",
    "\n",
    "# vocabulary size\n",
    "vocabulary_size = len(vocab_processor.vocabulary_)\n",
    "print 'Vocabulary_size: ', vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#notes_processed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question?\n",
    "VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV   \n",
    "what do we do if the test data has a document with a bigger length than the max for the padding? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transforming to embeddings using word2vec\n",
    "\n",
    "From: \"A Comparison of Rule-Based and Deep Learning Models for Patient Phenotyping\"\n",
    "\n",
    "\"We pre-train our embeddings with word2vec on all discharge notes available in the MIMIC-III database.   \n",
    "The word embeddings of all words in the text to classify are concatenated and used as input to the\n",
    "convolutional layer. Convolutions detect a signal from a combination of adjacent inputs. We\n",
    "combine multiple convolutions of different lengths to evaluate phrases that are anywhere from\n",
    "two to five words long,\"   \n",
    "\n",
    "(tf-idf is removing negations..  embedding is taking care of mispellings.. we may need further training-tuning because of medical terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://code.google.com/archive/p/word2vec/\n",
    "    \n",
    "Pre-trained word and phrase vectors\n",
    "\n",
    "\"We are publishing pre-trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in [2]. The archive is available here: GoogleNews-vectors-negative300.bin.gz.\"   \n",
    "\n",
    "### for now we wil train our own embeddings, but word2vec will be better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set samples: 7000\n",
      "Dev set samples: 1500\n",
      "Test set samples: 1500\n"
     ]
    }
   ],
   "source": [
    "def split_file(data, train_frac = 0.7, dev_frac = 0.15):   \n",
    "    train_split_idx = int(train_frac * len(data))\n",
    "    dev_split_idx = int ((train_frac + dev_frac)* len(data))\n",
    "    train_data = data[:train_split_idx]\n",
    "    dev_data = data[train_split_idx:dev_split_idx]\n",
    "    test_data = data[dev_split_idx:]\n",
    "    return train_data, dev_data, test_data\n",
    "\n",
    "\n",
    "train_notes, dev_notes, test_notes = split_file (notes_ids)\n",
    "train_labels, dev_labels, test_labels = split_file (labels_vector)\n",
    "print 'Training set samples:', len (train_notes)\n",
    "print 'Dev set samples:', len (dev_notes)\n",
    "print 'Test set samples:', len (test_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Training\n",
    "\n",
    "here is an example of a CNN to classify text.. our model will have different values for d (embedding-size, region sizes, etc)\n",
    "<img src=\"CNN_for_text2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the CNN used with the MIMIC discharge summaries\n",
    "<img src=\"mimic_CNN_text_classification.png\"/>\n",
    "\n",
    "\n",
    "\"For the CNN model, we used 100 filters for each of the widths 2, 3, 4, and 5.    \n",
    "To prevent overfitting, we set the dropout probability to 0.5 and used L2-normalization to normalize word\n",
    "embeddings to have a max norm of 3.64     \n",
    "The model was trained using adadelta with an initial learning rate of 1 for 20 epochs.   \n",
    "The CNN model was implemented using Lua and the Torch7 framework.66    \n",
    "All baseline models were implemented using Python with the scikit-learn library.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### sources:\n",
    "http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/  \n",
    "http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/   \n",
    "https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py   \n",
    "https://www.tensorflow.org/get_started/mnist/pros   \n",
    "https://www.tensorflow.org/api_docs/python/tf/nn/conv2d   \n",
    " \n",
    " multi-label\n",
    " https://github.com/may-/cnn-re-tf/blob/master/cnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: \"A Comparison of Rule-Based and Deep Learning Models for Patient Phenotyping\"\n",
    "\n",
    "\"For the CNN model, we used 100 filters for each of the widths 2, 3, 4, and 5.   \n",
    "To prevent overfitting, we set the dropout probability to 0.5 and used L2-normalization to normalize word\n",
    "embeddings to have a max norm of 3.64   \n",
    "The model was trained using adadelta with an initial learning rate of 1 for 20 epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, X, y, batch_size, dropout_keep_prob):\n",
    "    for batch in xrange(0, X.shape[0], batch_size):\n",
    "        # x SHAPE:   [batch_size, sequence_length, embedding_size]\n",
    "        X_batch = X[batch : batch + batch_size]\n",
    "        y_batch = y[batch : batch + batch_size]\n",
    "        feed_dict = {lm.input_x:X_batch,lm.input_y:y_batch,lm.dropout_keep_prob: dropout_keep_prob}\n",
    "        #loss, train_op_value =  session.run( [lm.loss,lm.train],feed_dict=feed_dict ) \n",
    "        loss, _, step = session.run([lm.loss, lm.train_op, lm.global_step], feed_dict)\n",
    "        if batch % 500: \n",
    "            print 'batch: %d, loss: %5.5f' % (batch, loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_icd9_codes(lm, session, x_data, y_data, dropout_keep_prob=1.0):\n",
    "    total_y_hat = []\n",
    "    for batch in xrange(0, x_data.shape[0], batch_size):\n",
    "        X_batch = x_data[batch : batch + batch_size]\n",
    "        Y_batch = y_data[batch : batch + batch_size]\n",
    "        y_hat_out = session.run(lm.y_hat, feed_dict={lm.input_x:X_batch,lm.input_y:Y_batch, lm.dropout_keep_prob: dropout_keep_prob})\n",
    "        total_y_hat.extend(y_hat_out)\n",
    "    return  total_y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build tensorflow graphs\n",
    "reload(cnn_model)\n",
    "\n",
    "# Model parameters\n",
    "\n",
    "model_params = dict(vocab_size= vocabulary_size, sequence_length=max_document_length, learning_rate=0.0001,\\\n",
    "                    embedding_size=128, num_classes=20, filter_sizes=[2,3,4,5], num_filters=100)\n",
    "\n",
    "# Build and Train Model\n",
    "cnn = cnn_model.NNLM(**model_params)\n",
    "cnn.BuildCoreGraph()\n",
    "cnn.BuildTrainGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TF_SAVEDIR = \"tf_saved\"\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"cnn_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_num: 0\n",
      "batch: 50, loss: 31.57923\n",
      "batch: 100, loss: 29.25520\n",
      "batch: 150, loss: 27.83793\n",
      "batch: 200, loss: 28.33652\n",
      "batch: 250, loss: 33.48518\n",
      "batch: 300, loss: 25.25622\n",
      "batch: 350, loss: 26.20545\n",
      "batch: 400, loss: 25.39668\n",
      "batch: 450, loss: 24.38098\n",
      "batch: 550, loss: 25.61077\n",
      "batch: 600, loss: 26.08123\n",
      "batch: 650, loss: 27.62245\n",
      "batch: 700, loss: 20.53951\n",
      "batch: 750, loss: 22.85826\n",
      "batch: 800, loss: 23.02357\n",
      "batch: 850, loss: 21.80454\n",
      "batch: 900, loss: 21.93460\n",
      "batch: 950, loss: 23.94118\n",
      "batch: 1050, loss: 21.81063\n",
      "batch: 1100, loss: 23.11218\n",
      "batch: 1150, loss: 18.13833\n",
      "batch: 1200, loss: 19.88521\n",
      "batch: 1250, loss: 22.96114\n",
      "batch: 1300, loss: 19.01944\n",
      "batch: 1350, loss: 19.21716\n",
      "batch: 1400, loss: 20.62275\n",
      "batch: 1450, loss: 20.03189\n",
      "batch: 1550, loss: 21.14845\n",
      "batch: 1600, loss: 15.45201\n",
      "batch: 1650, loss: 19.25917\n",
      "batch: 1700, loss: 14.29665\n",
      "batch: 1750, loss: 20.30489\n",
      "batch: 1800, loss: 16.68016\n",
      "batch: 1850, loss: 17.68271\n",
      "batch: 1900, loss: 16.53237\n",
      "batch: 1950, loss: 15.95314\n",
      "batch: 2050, loss: 15.51072\n",
      "batch: 2100, loss: 18.18981\n",
      "batch: 2150, loss: 14.63490\n",
      "batch: 2200, loss: 16.04677\n",
      "batch: 2250, loss: 16.65072\n",
      "batch: 2300, loss: 14.33055\n",
      "batch: 2350, loss: 17.24294\n",
      "batch: 2400, loss: 15.54059\n",
      "batch: 2450, loss: 15.32136\n",
      "batch: 2550, loss: 16.93499\n",
      "batch: 2600, loss: 14.88696\n",
      "batch: 2650, loss: 14.77677\n",
      "batch: 2700, loss: 14.97573\n",
      "batch: 2750, loss: 14.92811\n",
      "batch: 2800, loss: 16.56739\n",
      "batch: 2850, loss: 18.41746\n",
      "batch: 2900, loss: 14.41423\n",
      "batch: 2950, loss: 15.21894\n",
      "batch: 3050, loss: 14.93760\n",
      "batch: 3100, loss: 15.88780\n",
      "batch: 3150, loss: 16.18545\n",
      "batch: 3200, loss: 15.95709\n",
      "batch: 3250, loss: 15.75107\n",
      "batch: 3300, loss: 13.99182\n",
      "batch: 3350, loss: 15.00692\n",
      "batch: 3400, loss: 15.27590\n",
      "batch: 3450, loss: 17.02261\n",
      "batch: 3550, loss: 14.34730\n",
      "batch: 3600, loss: 16.79064\n",
      "batch: 3650, loss: 15.54140\n",
      "batch: 3700, loss: 16.77999\n",
      "batch: 3750, loss: 15.10662\n",
      "batch: 3800, loss: 14.51151\n",
      "batch: 3850, loss: 15.19576\n",
      "batch: 3900, loss: 15.26972\n",
      "batch: 3950, loss: 14.67046\n",
      "batch: 4050, loss: 18.97623\n",
      "batch: 4100, loss: 11.30645\n",
      "batch: 4150, loss: 16.76275\n",
      "batch: 4200, loss: 15.17008\n",
      "batch: 4250, loss: 16.45267\n",
      "batch: 4300, loss: 15.06910\n",
      "batch: 4350, loss: 18.08929\n",
      "batch: 4400, loss: 14.39661\n",
      "batch: 4450, loss: 13.12590\n",
      "batch: 4550, loss: 16.20221\n",
      "batch: 4600, loss: 16.63199\n",
      "batch: 4650, loss: 14.62280\n",
      "batch: 4700, loss: 15.38005\n",
      "batch: 4750, loss: 16.02670\n",
      "batch: 4800, loss: 17.05731\n",
      "batch: 4850, loss: 16.58923\n",
      "batch: 4900, loss: 14.77581\n",
      "batch: 4950, loss: 18.56059\n",
      "batch: 5050, loss: 17.39213\n",
      "batch: 5100, loss: 15.59759\n",
      "batch: 5150, loss: 12.91182\n",
      "batch: 5200, loss: 14.93277\n",
      "batch: 5250, loss: 14.88724\n",
      "batch: 5300, loss: 15.18817\n",
      "batch: 5350, loss: 14.90787\n",
      "batch: 5400, loss: 14.77351\n",
      "batch: 5450, loss: 13.85278\n",
      "batch: 5550, loss: 17.18856\n",
      "batch: 5600, loss: 17.34447\n",
      "batch: 5650, loss: 14.35447\n",
      "batch: 5700, loss: 14.28369\n",
      "batch: 5750, loss: 15.41415\n",
      "batch: 5800, loss: 15.08463\n",
      "batch: 5850, loss: 15.51584\n",
      "batch: 5900, loss: 17.56997\n",
      "batch: 5950, loss: 14.82215\n",
      "batch: 6050, loss: 14.22603\n",
      "batch: 6100, loss: 16.55454\n",
      "batch: 6150, loss: 16.97935\n",
      "batch: 6200, loss: 15.45904\n",
      "batch: 6250, loss: 13.08499\n",
      "batch: 6300, loss: 16.20872\n",
      "batch: 6350, loss: 14.28902\n",
      "batch: 6400, loss: 13.95721\n",
      "batch: 6450, loss: 18.28862\n",
      "batch: 6550, loss: 14.82957\n",
      "batch: 6600, loss: 15.60366\n",
      "batch: 6650, loss: 15.37600\n",
      "batch: 6700, loss: 15.57448\n",
      "batch: 6750, loss: 18.84041\n",
      "batch: 6800, loss: 13.43505\n",
      "batch: 6850, loss: 19.01012\n",
      "batch: 6900, loss: 15.84168\n",
      "batch: 6950, loss: 18.01208\n",
      "predicting training now \n",
      "predicting dev set now\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "num_epochs = 1\n",
    "training_dropout_keep_prob = 0.9\n",
    "\n",
    "with cnn.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=cnn.graph) as session:\n",
    "    session.run(initializer)\n",
    "    #training\n",
    "    for epoch_num in xrange(num_epochs):\n",
    "        print 'epoch_num:' , epoch_num\n",
    "        run_epoch(cnn, session, train_notes, train_labels, batch_size,dropout_keep_prob=training_dropout_keep_prob )\n",
    "    saver.save(session, trained_filename)\n",
    "    print 'predicting training now '\n",
    "    train_y_hat = predict_icd9_codes(cnn, session, train_notes, train_labels)   \n",
    "    print 'predicting dev set now'\n",
    "    dev_y_hat = predict_icd9_codes(cnn, session, dev_notes, dev_labels)\n",
    "    print 'done!'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[  2.46458929e-02   8.92440148e-04   6.43655658e-02   1.19024195e-01\n",
      "   1.33374035e-01   1.03435945e-03   4.27908218e-03   3.11144213e-05\n",
      "   1.17115835e-02   3.58440578e-01   2.58575082e-02   2.67258827e-02\n",
      "   3.76946409e-04   2.04408169e-02   1.24265395e-01   1.46946609e-02\n",
      "   1.26701919e-03   5.47282072e-03   6.12568064e-03   5.69743961e-02]\n",
      "0.999999973264\n"
     ]
    }
   ],
   "source": [
    "print train_labels[0]\n",
    "print train_y_hat[0]\n",
    "print sum (train_y_hat[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ranking loss:  0.345096664335\n",
      "Development ranking loss:  0.352917257167\n"
     ]
    }
   ],
   "source": [
    "# ranking loss\n",
    "training_ranking_loss = label_ranking_loss(train_labels, train_y_hat)\n",
    "print \"Training ranking loss: \", training_ranking_loss\n",
    "dev_ranking_loss = label_ranking_loss(dev_labels, dev_y_hat)\n",
    "print \"Development ranking loss: \", dev_ranking_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO  create a model for thresholding\n",
    "\n",
    "Large-scale Multi-label Text Classificationâ€”Revisiting Neural Networks\n",
    "\n",
    "\n",
    "\"3.3 Thresholding\n",
    "Once training of the neural network is finished, its output may be interpreted as a probability\n",
    "distribution p (ojx) over the labels for a given document x. The probability distribution\n",
    "can be used to rank labels, but additional measures are needed in order to split\n",
    "the ranking into relevant and irrelevant labels. For transforming the ranked list of labels\n",
    "into a set of binary predictions, we train a multi-label threshold predictor from training\n",
    "data. This sort of thresholding methods are also used in [6, 31]\n",
    "For each document xm, labels are sorted by the probabilities in decreasing order.\n",
    "Ideally, if NNs successfully learn a mapping function f , all correct (positive) labels\n",
    "will be placed on top of the sorted list and there should be large margin between the set\n",
    "of positive labels and the set of negative labels. Using F1 score as a reference measure,\n",
    "we calculate classification performances at every pair of successive positive labels and\n",
    "choose a threshold value tm that produces the best performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_f1_score(y_true,y_hat,threshold, average):\n",
    "    hot_y = np.where(np.array(y_hat) > threshold, 1, 0)\n",
    "    return f1_score(np.array(y_true), hot_y, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores\n",
      "threshold | training | dev  \n",
      "0.005:      0.321      0.317\n",
      "0.010:      0.334      0.331\n",
      "0.020:      0.347      0.345\n",
      "0.030:      0.351      0.350\n",
      "0.040:      0.349      0.344\n",
      "0.050:      0.342      0.337\n",
      "0.055:      0.340      0.334\n",
      "0.058:      0.337      0.332\n",
      "0.060:      0.335      0.330\n",
      "0.070:      0.324      0.320\n",
      "0.080:      0.313      0.308\n",
      "0.100:      0.292      0.283\n",
      "0.500:      0.046      0.043\n"
     ]
    }
   ],
   "source": [
    "print 'F1 scores'\n",
    "print 'threshold | training | dev  '\n",
    "f1_score_average = 'micro'\n",
    "for threshold in [ 0.005, 0.01,0.02,0.03,0.04,0.05,0.055,0.058,0.06, 0.07, 0.08, 0.1, 0.5]:\n",
    "    train_f1 = get_f1_score(train_labels, train_y_hat,threshold,f1_score_average)\n",
    "    dev_f1 = get_f1_score(dev_labels, dev_y_hat,threshold,f1_score_average)\n",
    "    print '%1.3f:      %1.3f      %1.3f' % (threshold,train_f1, dev_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "10,000 records, 1 epoch\n",
    "adam optimizer\n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.005:      0.321      0.317\n",
    "0.010:      0.334      0.331\n",
    "0.020:      0.347      0.345\n",
    "0.030:      0.351      0.350\n",
    "0.040:      0.349      0.344\n",
    "0.050:      0.342      0.337\n",
    "0.055:      0.340      0.334\n",
    "0.058:      0.337      0.332\n",
    "0.060:      0.335      0.330\n",
    "0.070:      0.324      0.320\n",
    "0.080:      0.313      0.308\n",
    "0.100:      0.292      0.283\n",
    "0.500:      0.046      0.043\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adam optimizer with learning rate 0.0001,dropout= 0.9\n",
    "\n",
    "```\n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.005:      0.298      0.292\n",
    "0.010:      0.291      0.291\n",
    "0.020:      0.304      0.301\n",
    "0.030:      0.313      0.309\n",
    "0.040:      0.323      0.307\n",
    "0.050:      0.328      0.305\n",
    "0.055:      0.325      0.301\n",
    "0.058:      0.325      0.297\n",
    "0.060:      0.327      0.294\n",
    "0.070:      0.324      0.288\n",
    "0.080:      0.316      0.275\n",
    "0.100:      0.306      0.264\n",
    "0.500:      0.007      0.004\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1000 notes, top 20 labels, adadelta optimizer (but it goes wild on epoch #13) \n",
    "learning rate = 0.5, training-dropout = 1.0, batch_size = 50, num_epochs = 5\n",
    "\n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.005:      0.315      0.311\n",
    "0.010:      0.337      0.323\n",
    "0.020:      0.367      0.342\n",
    "0.030:      0.391      0.337\n",
    "0.040:      0.406      0.346\n",
    "0.050:      0.417      0.353\n",
    "0.055:      0.420      0.343\n",
    "0.058:      0.420      0.343\n",
    "0.060:      0.421      0.343\n",
    "0.070:      0.414      0.340\n",
    "0.080:      0.411      0.332\n",
    "0.100:      0.393      0.312\n",
    "0.500:      0.040      0.034\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1000 notes, top 20 labels, adadelta optimizer (goes wild on #epoch 13)\n",
    "learning rate = 0.5, training-dropout = 0.5, batch_size = 50, num_epochs = 5\n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.005:      0.375      0.362\n",
    "0.010:      0.382      0.364\n",
    "0.020:      0.378      0.356\n",
    "0.030:      0.352      0.342\n",
    "0.040:      0.331      0.324\n",
    "0.050:      0.319      0.324\n",
    "0.060:      0.306      0.312\n",
    "0.100:      0.278      0.294\n",
    "0.500:      0.200      0.20\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Thoughts so far\n",
    "\n",
    "The CNN loss gets stuck with dropout_keep = 0.5.. \n",
    "I change it to 0.9, no overfitting, but the dev F1 score of 36%,which is just 1% hihter than the baseline model that always predict the top 4 most common icd-9 code and to the NN Baseline.\n",
    "\n",
    "There is a LSTM model by this paper: \"Applying Deep Learning to ICD-9 Multi-label Classification from Medical Records\" which did achieve a 42% F1-score. (https://cs224d.stanford.edu/reports/priyanka.pdf), but it only uses the top 10 icd9 codes.\n",
    "\n",
    "(note:  I think the CNN should be getting about 40% also)\n",
    "\n",
    "The \"A Comparison of Rule-Based and Deep Learning Models for Patient Phenotyping\"  study did get a 70% F1-score, but they don't use the icd9-labels but phenotypes labels they annotated themselved (via a group of medical professionals). (https://arxiv.org/abs/1703.08705). There were ONLY 10 phenotypes.\n",
    "\n",
    "The discharge summaries are labeled with ICD9-codes that are leaves in the ICD9-hierarchy (which has hundreds of ICD9-codes), then maybe these leave nodes are too specific and difficult to predict, one experiment would be to replaced all the ICD9-codes with their parent in the second or third level in the hierarchy and see if predictions work better that way.   \n",
    "\n",
    "### Lessons learned: \n",
    "* Adadelta optimizer has problems when running more than 10 epochs, the training loss stops going down and instead goes upd wildly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using Keras\n",
    "base on example: https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set parameters:\n",
    "embedding_dims = 128\n",
    "filters = 100\n",
    "maxlen = max_document_length\n",
    "\n",
    "max_features = 5000\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "#model.add(Embedding(max_features,embedding_dims,input_length=maxlen))\n",
    "\n",
    "#model.add(Dropout(0.2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
