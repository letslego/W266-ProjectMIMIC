{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import *\n",
    "import re\n",
    "from tensorflow.contrib import learn\n",
    "import sys, os\n",
    "import tensorflow as tf\n",
    "import cnn_model\n",
    "\n",
    "from sklearn.metrics import label_ranking_loss\n",
    "from sklearn.metrics import f1_score\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Sources:\n",
    "http://ruder.io/deep-learning-nlp-best-practices/index.html#classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the dataset:  45837\n"
     ]
    }
   ],
   "source": [
    "#with open('../../../psql_files/disch_notes_all_icd9.csv', 'rb') as csvfile:\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "with open('../baseline/psql_files/dis_notes_icd9.csv', 'rb') as csvfile:\n",
    "    discharge_notes_reader = csv.reader(csvfile)\n",
    "    discharge_notes_list = list(discharge_notes_reader)    \n",
    "random.shuffle(discharge_notes_list)\n",
    "\n",
    "print \"Number of records in the dataset: \", len (discharge_notes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will take only 10,000 records to compare with NN baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting for 1,000 just for programming\n",
    "number_records = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of discharge clinical notes:  1000\n"
     ]
    }
   ],
   "source": [
    "discharge_notes_icd9 = np.asarray(discharge_notes_list[0:number_records])\n",
    "print 'Number of discharge clinical notes: ', len(discharge_notes_icd9)\n",
    "discharge_notes= discharge_notes_icd9[:,3]\n",
    "discharge_labels = discharge_notes_icd9[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats about Notes  (TODO:)\n",
    "* vocabulary of size\n",
    "* find out notes that are too large, outliers to take out (otherwise the embeddings will pad a lot of zeroes to the other note-vectors("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting icd9 labels to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transforming list of icd_codes into a vector\n",
    "def get_icd9_array(icd9_codes):\n",
    "    icd9_index_array = [0]*len(unique_icd9_codes)\n",
    "    for icd9_code in icd9_codes.split():\n",
    "        index = icd9_to_id [icd9_code]\n",
    "        icd9_index_array[index] = 1\n",
    "    return icd9_index_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'4019': 463, '42731': 304, '4280': 290, '41401': 282, '2724': 185, '25000': 181, '5849': 181, '51881': 154, '5990': 142, '2720': 141, '53081': 132, '2859': 128, '486': 120, '496': 109, '2449': 101, '2762': 97, '2851': 95, '5070': 93, '0389': 87, '99592': 69})\n",
      "  \n",
      "List of unique icd9 codes from all labels:  ['2859', '4280', '2724', '25000', '99592', '2851', '2762', '2449', '4019', '0389', '41401', '53081', '51881', '2720', '42731', '486', '496', '5070', '5849', '5990']\n"
     ]
    }
   ],
   "source": [
    "#counts by icd9_codes\n",
    "icd9_codes = Counter()\n",
    "for label in discharge_labels:\n",
    "    for icd9_code in label.split():\n",
    "        icd9_codes[icd9_code] += 1\n",
    "print icd9_codes\n",
    "\n",
    "# list of unique icd9_codes and lookups for its index in the vector\n",
    "unique_icd9_codes = list (icd9_codes)\n",
    "index_to_icd9 = dict(enumerate(unique_icd9_codes))\n",
    "icd9_to_id = {v:k for k,v in index_to_icd9.iteritems()}\n",
    "print '  '\n",
    "print 'List of unique icd9 codes from all labels: ', unique_icd9_codes\n",
    "\n",
    "#convert icd9 codes into ids\n",
    "labels_vector= list(map(get_icd9_array,discharge_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "\n",
    "\n",
    "(1) Clean the text data using the same code as the original paper.\n",
    "https://github.com/yoonkim/CNN_sentence\n",
    "\n",
    "(2) Pad each note to the maximum note length, which turns out to be NN. We append special <PAD> tokens to all other notes to make them NN words. Padding sentences to the same length is useful because it allows us to efficiently batch our data since each example in a batch must be of the same length.\n",
    "(3) Build a vocabulary index and map each word to an integer between 0 and 18,765 (the vocabulary size). Each sentence becomes a vector of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def note_preprocessing(data_notes):\n",
    "    notes_stripped = [s.strip() for s in data_notes]\n",
    "    notes_clean = [clean_str(note) for note in notes_stripped ]\n",
    "    # Build vocabulary\n",
    "    note_words_length =  [len(x.split(\" \")) for x in notes_clean]\n",
    "    max_document_length = max( note_words_length)  \n",
    "    average_length = np.mean(note_words_length)\n",
    "    return max_document_length, average_length, notes_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " max document length:  7884\n",
      "average document length:  1901.717\n",
      "Vocabulary_size:  28609\n"
     ]
    }
   ],
   "source": [
    "#preprocess documents\n",
    "max_document_length, average_document_length, notes_processed = note_preprocessing(discharge_notes)\n",
    "\n",
    "\n",
    "print ' max document length: ', max_document_length\n",
    "print 'average document length: ', average_document_length\n",
    "\n",
    "#create vocabulary processor\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    \n",
    "# convert words to ids, and each document is padded\n",
    "notes_ids = np.array(list(vocab_processor.fit_transform(notes_processed)))\n",
    "\n",
    "# vocabulary size\n",
    "vocabulary_size = len(vocab_processor.vocabulary_)\n",
    "print 'Vocabulary_size: ', vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question?\n",
    "VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV   \n",
    "what do we do if the test data has a document with a bigger length than the max for the padding? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transforming to embeddings using word2vec\n",
    "\n",
    "From: \"A Comparison of Rule-Based and Deep Learning Models for Patient Phenotyping\"\n",
    "\n",
    "\"We pre-train our embeddings with word2vec on all discharge notes available in the MIMIC-III database.   \n",
    "The word embeddings of all words in the text to classify are concatenated and used as input to the\n",
    "convolutional layer. Convolutions detect a signal from a combination of adjacent inputs. We\n",
    "combine multiple convolutions of different lengths to evaluate phrases that are anywhere from\n",
    "two to five words long,\"   \n",
    "\n",
    "(tf-idf is removing negations..  embedding is taking care of mispellings.. we may need further training-tuning because of medical terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://code.google.com/archive/p/word2vec/\n",
    "    \n",
    "Pre-trained word and phrase vectors\n",
    "\n",
    "\"We are publishing pre-trained vectors trained on part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in [2]. The archive is available here: GoogleNews-vectors-negative300.bin.gz.\"   \n",
    "\n",
    "### for now we wil train our own embeddings, but word2vec will be better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set samples: 700\n",
      "Dev set samples: 150\n",
      "Test set samples: 150\n"
     ]
    }
   ],
   "source": [
    "def split_file(data, train_frac = 0.7, dev_frac = 0.15):   \n",
    "    train_split_idx = int(train_frac * len(data))\n",
    "    dev_split_idx = int ((train_frac + dev_frac)* len(data))\n",
    "    train_data = data[:train_split_idx]\n",
    "    dev_data = data[train_split_idx:dev_split_idx]\n",
    "    test_data = data[dev_split_idx:]\n",
    "    return train_data, dev_data, test_data\n",
    "\n",
    "\n",
    "train_notes, dev_notes, test_notes = split_file (notes_ids)\n",
    "train_labels, dev_labels, test_labels = split_file (labels_vector)\n",
    "print 'Training set samples:', len (train_notes)\n",
    "print 'Dev set samples:', len (dev_notes)\n",
    "print 'Test set samples:', len (test_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Training\n",
    "\n",
    "here is an example of a CNN to classify text.. our model will have different values for d (embedding-size, region sizes, etc)\n",
    "<img src=\"CNN_for_text2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the CNN used with the MIMIC discharge summaries\n",
    "<img src=\"mimic_CNN_text_classification.png\"/>\n",
    "\n",
    "\n",
    "\"For the CNN model, we used 100 filters for each of the widths 2, 3, 4, and 5.    \n",
    "To prevent overfitting, we set the dropout probability to 0.5 and used L2-normalization to normalize word\n",
    "embeddings to have a max norm of 3.64     \n",
    "The model was trained using adadelta with an initial learning rate of 1 for 20 epochs.   \n",
    "The CNN model was implemented using Lua and the Torch7 framework.66    \n",
    "All baseline models were implemented using Python with the scikit-learn library.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### sources:\n",
    "http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/  \n",
    "http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/   \n",
    "https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py   \n",
    "https://www.tensorflow.org/get_started/mnist/pros   \n",
    "https://www.tensorflow.org/api_docs/python/tf/nn/conv2d   \n",
    " \n",
    " multi-label\n",
    " https://github.com/may-/cnn-re-tf/blob/master/cnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: \"A Comparison of Rule-Based and Deep Learning Models for Patient Phenotyping\"\n",
    "\n",
    "\"For the CNN model, we used 100 filters for each of the widths 2, 3, 4, and 5.   \n",
    "To prevent overfitting, we set the dropout probability to 0.5 and used L2-normalization to normalize word\n",
    "embeddings to have a max norm of 3.64   \n",
    "The model was trained using adadelta with an initial learning rate of 1 for 20 epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, X, y, batch_size):\n",
    "    for batch in xrange(0, X.shape[0], batch_size):\n",
    "        # x SHAPE:   [batch_size, sequence_length, embedding_size]\n",
    "        print 'running batch: ', batch \n",
    "        X_batch = X[batch : batch + batch_size]\n",
    "        y_batch = y[batch : batch + batch_size]\n",
    "        feed_dict = {lm.input_x:X_batch,lm.input_y:y_batch,lm.dropout_keep_prob:0.5}\n",
    "        #loss, train_op_value =  session.run( [lm.loss,lm.train],feed_dict=feed_dict ) \n",
    "        loss, _, step = session.run([lm.loss, lm.train_op, lm.global_step], feed_dict)\n",
    "        print 'loss: ', loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_icd9_codes(lm, session, x_data, y_data):\n",
    "    total_y_hat = []\n",
    "    for batch in xrange(0, x_data.shape[0], batch_size):\n",
    "        X_batch = x_data[batch : batch + batch_size]\n",
    "        Y_batch = y_data[batch : batch + batch_size]\n",
    "        y_hat_out = session.run(lm.y_hat, feed_dict={lm.input_x:X_batch,lm.input_y:Y_batch, lm.dropout_keep_prob: 1.0})\n",
    "        total_y_hat.extend(y_hat_out)\n",
    "    return  total_y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build tensorflow graphs\n",
    "reload(cnn_model)\n",
    "\n",
    "# Model parameters\n",
    "\n",
    "model_params = dict(vocab_size= vocabulary_size, sequence_length=max_document_length, learning_rate=1.0,\\\n",
    "                    embedding_size=128, num_classes=20, filter_sizes=[2,3,4,5], num_filters=100)\n",
    "\n",
    "# Build and Train Model\n",
    "cnn = cnn_model.NNLM(**model_params)\n",
    "cnn.BuildCoreGraph()\n",
    "cnn.BuildTrainGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TF_SAVEDIR = \"tf_saved\"\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"cnn_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_num: 0\n",
      "running batch:  0\n",
      "loss:  43.7063\n",
      "running batch:  50\n",
      "loss:  36.9216\n",
      "running batch:  100\n",
      "loss:  39.4771\n",
      "running batch:  150\n",
      "loss:  34.7219\n",
      "running batch:  200\n",
      "loss:  40.6437\n",
      "running batch:  250\n",
      "loss:  46.2586\n",
      "running batch:  300\n",
      "loss:  50.0105\n",
      "running batch:  350\n",
      "loss:  43.3751\n",
      "running batch:  400\n",
      "loss:  34.8892\n",
      "running batch:  450\n",
      "loss:  36.6\n",
      "running batch:  500\n",
      "loss:  32.0268\n",
      "running batch:  550\n",
      "loss:  35.395\n",
      "running batch:  600\n",
      "loss:  37.2548\n",
      "running batch:  650\n",
      "loss:  31.4232\n",
      "epoch_num: 1\n",
      "running batch:  0\n",
      "loss:  42.9412\n",
      "running batch:  50\n",
      "loss:  34.9633\n",
      "running batch:  100\n",
      "loss:  32.7634\n",
      "running batch:  150\n",
      "loss:  30.7938\n",
      "running batch:  200\n",
      "loss:  42.8349\n",
      "running batch:  250\n",
      "loss:  44.2636\n",
      "running batch:  300\n",
      "loss:  44.7367\n",
      "running batch:  350\n",
      "loss:  38.2166\n",
      "running batch:  400\n",
      "loss:  32.2232\n",
      "running batch:  450\n",
      "loss:  33.3221\n",
      "running batch:  500\n",
      "loss:  32.1199\n",
      "running batch:  550\n",
      "loss:  30.9009\n",
      "running batch:  600\n",
      "loss:  34.2838\n",
      "running batch:  650\n",
      "loss:  30.1417\n",
      "epoch_num: 2\n",
      "running batch:  0\n",
      "loss:  42.9206\n",
      "running batch:  50\n",
      "loss:  38.6935\n",
      "running batch:  100\n",
      "loss:  34.0035\n",
      "running batch:  150\n",
      "loss:  31.6629\n",
      "running batch:  200\n",
      "loss:  41.6267\n",
      "running batch:  250\n",
      "loss:  46.1405\n",
      "running batch:  300\n",
      "loss:  46.3488\n",
      "running batch:  350\n",
      "loss:  41.5637\n",
      "running batch:  400\n",
      "loss:  36.0673\n",
      "running batch:  450\n",
      "loss:  38.3099\n",
      "running batch:  500\n",
      "loss:  36.8126\n",
      "running batch:  550\n",
      "loss:  33.0087\n",
      "running batch:  600\n",
      "loss:  38.0347\n",
      "running batch:  650\n",
      "loss:  35.5242\n",
      "predicting training now \n",
      "predicting dev set now\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "num_epochs = 3\n",
    "\n",
    "\n",
    "with cnn.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=cnn.graph) as session:\n",
    "    session.run(initializer)\n",
    "    #training\n",
    "    for epoch_num in xrange(num_epochs):\n",
    "        print 'epoch_num:' , epoch_num\n",
    "        run_epoch(cnn, session, train_notes, train_labels, batch_size)\n",
    "    saver.save(session, trained_filename)\n",
    "    print 'predicting training now '\n",
    "    train_y_hat = predict_icd9_codes(cnn, session, train_notes, train_labels)   \n",
    "    print 'predicting dev set now'\n",
    "    dev_y_hat = predict_icd9_codes(cnn, session, dev_notes, dev_labels)\n",
    "    print 'done!'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ranking loss:  0.318951479003\n",
      "Development ranking loss:  0.331997186708\n"
     ]
    }
   ],
   "source": [
    "# ranking loss\n",
    "training_ranking_loss = label_ranking_loss(train_labels, train_y_hat)\n",
    "print \"Training ranking loss: \", training_ranking_loss\n",
    "dev_ranking_loss = label_ranking_loss(dev_labels, dev_y_hat)\n",
    "print \"Development ranking loss: \", dev_ranking_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_f1_score(y_true,y_hat,threshold, average):\n",
    "    hot_y = np.where(np.array(y_hat) > threshold, 1, 0)\n",
    "    return f1_score(np.array(y_true), hot_y, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores\n",
      "threshold | training | dev  \n",
      "0.005:      0.373      0.361\n",
      "0.010:      0.376      0.348\n",
      "0.020:      0.368      0.343\n",
      "0.030:      0.363      0.343\n",
      "0.040:      0.359      0.335\n",
      "0.050:      0.353      0.330\n",
      "0.060:      0.352      0.329\n",
      "0.100:      0.337      0.320\n",
      "0.500:      0.146      0.133\n"
     ]
    }
   ],
   "source": [
    "print 'F1 scores'\n",
    "print 'threshold | training | dev  '\n",
    "f1_score_average = 'micro'\n",
    "for threshold in [ 0.005, 0.01,0.02,0.03,0.04,0.05,0.06, 0.1, 0.5]:\n",
    "    train_f1 = get_f1_score(train_labels, train_y_hat,threshold,f1_score_average)\n",
    "    dev_f1 = get_f1_score(dev_labels, dev_y_hat,threshold,f1_score_average)\n",
    "    print '%1.3f:      %1.3f      %1.3f' % (threshold,train_f1, dev_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
