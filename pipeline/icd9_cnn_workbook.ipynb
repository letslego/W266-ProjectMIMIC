{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "#keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "# Custom functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import database_selection\n",
    "import vectorization\n",
    "import helpers\n",
    "import icd9_cnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/disch_notes_all_icd9.csv',\n",
    "                 names = ['HADM_ID', 'SUBJECT_ID', 'DATE', 'ICD9','TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_TOP = 20 \n",
    "full_df, top_codes = database_selection.filter_top_codes(df, 'ICD9', N_TOP, filter_empty = True)\n",
    "df = full_df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preprocess icd9 codes\n",
    "labels = vectorization.vectorize_icd_column(df, 'ICD9', top_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zenla/anaconda2/lib/python2.7/site-packages/pandas/core/generic.py:2773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 22476\n",
      "Average note length: 1748.878\n",
      "Max note length: 5641\n",
      "Final Vocabulary: 22476\n",
      "Final Max Sequence Length: 5000\n"
     ]
    }
   ],
   "source": [
    "#preprocess notes\n",
    "MAX_VOCAB = None # to limit original number of words (None if no limit)\n",
    "MAX_SEQ_LENGTH = 5000 # to limit length of word sequence (None if no limit)\n",
    "df.TEXT = vectorization.clean_notes(df, 'TEXT')\n",
    "data, dictionary, MAX_VOCAB = vectorization.vectorize_notes(df.TEXT, MAX_VOCAB, verbose = True)\n",
    "data, MAX_SEQ_LENGTH = vectorization.pad_notes(data, MAX_SEQ_LENGTH)\n",
    "print(\"Final Vocabulary: %s\" % MAX_VOCAB)\n",
    "print(\"Final Max Sequence Length: %s\" % MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train: ', (699, 5000), (699, 20))\n",
      "('Validation: ', (200, 5000), (200, 20))\n",
      "('Test: ', (101, 5000), (101, 20))\n"
     ]
    }
   ],
   "source": [
    "#split sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = helpers.train_val_test_split(\n",
    "    data, labels, val_size=0.2, test_size=0.1, random_state=101)\n",
    "print(\"Train: \", X_train.shape, y_train.shape)\n",
    "print(\"Validation: \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete temporary variables to free some memory\n",
    "del df, data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Vocabulary in notes:', 22476)\n",
      "('Vocabulary in original embedding:', 400000)\n",
      "('Vocabulary intersection:', 14345)\n"
     ]
    }
   ],
   "source": [
    "#creating embeddings\n",
    "EMBEDDING_LOC = '../data/glove.6B.100d.txt' # location of embedding\n",
    "EMBEDDING_DIM = 100 # given the glove that we chose\n",
    "EMBEDDING_MATRIX, embedding_dict = vectorization.embedding_matrix(EMBEDDING_LOC,\n",
    "                                                                  dictionary, EMBEDDING_DIM, verbose = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN for text classification\n",
    "\n",
    "Based on the following papers and links:\n",
    "* \"Convolutional Neural Networks for Sentence Classification\"   \n",
    "* \"A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification\"\n",
    "* http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "* https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras/blob/master/sentiment_cnn.py\n",
    "* http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "* https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reload(icd9_cnn_model)\n",
    "#### build model\n",
    "model = icd9_cnn_model.build_icd9_cnn_model (input_seq_length=MAX_SEQ_LENGTH, max_vocab = MAX_VOCAB,\n",
    "                             external_embeddings = True, embedding_trainable =True,\n",
    "                             embedding_dim=EMBEDDING_DIM,embedding_matrix=EMBEDDING_MATRIX,\n",
    "                             num_filters = 100, filter_sizes=[2,3,4,5],\n",
    "                             training_dropout_keep_prob=0.9,\n",
    "                             num_classes=N_TOP )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 699 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "48s - loss: 1.9359 - acc: 0.8041 - val_loss: 0.8085 - val_acc: 0.8207\n",
      "Epoch 2/5\n",
      "49s - loss: 1.6919 - acc: 0.8094 - val_loss: 0.7771 - val_acc: 0.8207\n",
      "Epoch 3/5\n",
      "49s - loss: 1.5507 - acc: 0.8093 - val_loss: 0.6493 - val_acc: 0.8238\n",
      "Epoch 4/5\n",
      "49s - loss: 1.3175 - acc: 0.8147 - val_loss: 0.6110 - val_acc: 0.8235\n",
      "Epoch 5/5\n",
      "49s - loss: 1.1382 - acc: 0.8146 - val_loss: 0.5787 - val_acc: 0.8235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fae58561610>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=50, epochs=5,\n",
    "validation_data=(X_val, y_val), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train, batch_size=50)\n",
    "pred_dev = model.predict(X_val, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores\n",
      "threshold | training | dev  \n",
      "0.020:      0.346      0.351\n",
      "0.030:      0.372      0.368\n",
      "0.040:      0.373      0.359\n",
      "0.050:      0.363      0.364\n",
      "0.055:      0.362      0.367\n",
      "0.058:      0.363      0.369\n",
      "0.060:      0.364      0.372\n",
      "0.080:      0.371      0.382\n",
      "0.100:      0.346      0.354\n",
      "0.200:      0.199      0.204\n",
      "0.300:      0.176      0.162\n",
      "0.500:      0.002      0.000\n"
     ]
    }
   ],
   "source": [
    "def get_f1_score(y_true,y_hat,threshold, average):\n",
    "    hot_y = np.where(np.array(y_hat) > threshold, 1, 0)\n",
    "    return f1_score(np.array(y_true), hot_y, average=average)\n",
    "\n",
    "print 'F1 scores'\n",
    "print 'threshold | training | dev  '\n",
    "f1_score_average = 'micro'\n",
    "for threshold in [ 0.02, 0.03,0.04,0.05,0.055,0.058,0.06, 0.08, 0.1,0.2,0.3, 0.5]:\n",
    "    train_f1 = get_f1_score(y_train, pred_train,threshold,f1_score_average)\n",
    "    dev_f1 = get_f1_score(y_val, pred_dev,threshold,f1_score_average)\n",
    "    print '%1.3f:      %1.3f      %1.3f' % (threshold,train_f1, dev_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO: model for Thresholding\n",
    "Papers about Thresholding:    \n",
    "\n",
    "* \"Convolutional Neural Network using a Threshold Predictor for Multi-label Speech Act Classification\"   \n",
    "* \"A Study on Threshold Selection for Multi-label Classification\"   \n",
    "\n",
    "These papers mention they use a model for thresholding, but it is not the main topic:   \n",
    "* \"A Review on Multi-Label Learning Algorithms\"   \n",
    "* \"Large-scale Multi-label Text Classification—Revisiting Neural Networks\"*   \n",
    "* \"A multi-label convolutional neural network for automatic image annotation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results with external embeddings = True , no additional training,  top 20\n",
    "```\n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.020:      0.337      0.329\n",
    "0.030:      0.360      0.353\n",
    "0.040:      0.365      0.374\n",
    "0.050:      0.372      0.375\n",
    "0.055:      0.370      0.377\n",
    "0.058:      0.369      0.375\n",
    "0.060:      0.368      0.375\n",
    "0.080:      0.348      0.361\n",
    "0.100:      0.309      0.319\n",
    "0.200:      0.198      0.208\n",
    "0.300:      0.157      0.138\n",
    "0.500:      0.000      0.000\n",
    "```\n",
    "\n",
    "### Results with external embeddings = False, top 20\n",
    "```\n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.020:      0.288      0.300\n",
    "0.030:      0.327      0.322\n",
    "0.040:      0.371      0.363\n",
    "0.050:      0.380      0.391\n",
    "0.055:      0.412      0.383\n",
    "0.058:      0.403      0.394\n",
    "0.060:      0.394      0.389\n",
    "0.080:      0.385      0.390\n",
    "0.100:      0.229      0.225\n",
    "0.200:      0.000      0.000\n",
    "0.300:      0.000      0.000\n",
    "0.500:      0.000      0.000\n",
    "```\n",
    "\n",
    "### Results with external embedding and training them , top 20\n",
    "```\n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.020:      0.334      0.333\n",
    "0.030:      0.362      0.360\n",
    "0.040:      0.366      0.374\n",
    "0.050:      0.373      0.380\n",
    "0.055:      0.374      0.382\n",
    "0.058:      0.376      0.376\n",
    "0.060:      0.376      0.378\n",
    "0.080:      0.387      0.371\n",
    "0.100:      0.366      0.350\n",
    "0.200:      0.179      0.171\n",
    "0.300:      0.020      0.020\n",
    "0.500:      0.000      0.000\n",
    "\n",
    "```\n",
    "\n",
    "### Results with external Embeddings = False, top 10, \n",
    "We can compare this setup with the LSTM published in the paper \"Applying Deep Learning to ICD-9 Multi-label Classification from Medical Records\", they got a F1-score of about 0.4168, we are getting 0.447\n",
    "\n",
    "``` \n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.020:      0.399      0.407\n",
    "0.030:      0.399      0.407\n",
    "0.040:      0.399      0.407\n",
    "0.050:      0.408      0.413\n",
    "0.055:      0.433      0.420\n",
    "0.058:      0.437      0.430\n",
    "0.060:      0.432      0.427\n",
    "0.080:      0.501      0.463\n",
    "0.100:      0.446      0.463\n",
    "0.200:      0.206      0.066\n",
    "0.300:      0.000      0.000\n",
    "0.500:      0.000      0.000\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Notes:\n",
    "\n",
    "\n",
    "(1) There is a LSTM model by this paper: \"Applying Deep Learning to ICD-9 Multi-label Classification from Medical Records\" which did achieve a 42% F1-score. (https://cs224d.stanford.edu/reports/priyanka.pdf), but it only uses the top 10 icd9 codes. We are getting 46% (just running with 1000 notes so far)\n",
    "\n",
    "\n",
    "(2) The \"A Comparison of Rule-Based and Deep Learning Models for Patient Phenotyping\"  study did get a 70% F1-score, but they don't use the icd9-labels but phenotypes labels they annotated themselved (via a group of medical professionals). (https://arxiv.org/abs/1703.08705). There were ONLY 10 phenotypes.\n",
    "\n",
    "The discharge summaries are labeled with ICD9-codes that are leaves in the ICD9-hierarchy (which has hundreds of ICD9-codes), then maybe these leave nodes are too specific and difficult to predict, one experiment would be to replaced all the ICD9-codes with their parent in the second or third level in the hierarchy and see if predictions work better that way.   \n",
    "\n",
    "(3) our baseline with top 20 codes had a f1-score of 35% (assigning top 4 icd9 codes to all notes, using a CNN with no external embeddings is getting about 40% f1-score.. a little better than the baseline\n",
    "\n",
    "(4) Papers published and best practices report  that external embeddings improve considerable the model's performance.. maybe it is not the case here because of the medical terms..   \n",
    "\n",
    "(5) Fixed Thresholding doesn't work well for multilabel classifications, we can implement a model to choose the appropriate threshold for each record (see notes above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
