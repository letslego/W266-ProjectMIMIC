{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Custom functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import database_selection\n",
    "import vectorization\n",
    "import helpers\n",
    "\n",
    "#keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/disch_notes_all_icd9.csv',\n",
    "                 names = ['HADM_ID', 'SUBJECT_ID', 'DATE', 'ICD9','TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_TOP = 20\n",
    "full_df, top_codes = database_selection.filter_top_codes(df, 'ICD9', N_TOP, filter_empty = True)\n",
    "df = full_df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preprocess icd9 codes\n",
    "labels = vectorization.vectorize_icd_column(df, 'ICD9', top_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 22476\n",
      "Average note length: 1748.878\n",
      "Max note length: 5641\n",
      "Final Vocabulary: 22476\n",
      "Final Max Sequence Length: 5000\n"
     ]
    }
   ],
   "source": [
    "#preprocess notes\n",
    "MAX_VOCAB = None # to limit original number of words (None if no limit)\n",
    "MAX_SEQ_LENGTH = 5000 # to limit length of word sequence (None if no limit)\n",
    "df.TEXT = vectorization.clean_notes(df, 'TEXT')\n",
    "data, dictionary, MAX_VOCAB = vectorization.vectorize_notes(df.TEXT, MAX_VOCAB, verbose = True)\n",
    "data, MAX_SEQ_LENGTH = vectorization.pad_notes(data, MAX_SEQ_LENGTH)\n",
    "print(\"Final Vocabulary: %s\" % MAX_VOCAB)\n",
    "print(\"Final Max Sequence Length: %s\" % MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train: ', (699, 5000), (699, 20))\n",
      "('Validation: ', (200, 5000), (200, 20))\n",
      "('Test: ', (101, 5000), (101, 20))\n"
     ]
    }
   ],
   "source": [
    "#split sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = helpers.train_val_test_split(\n",
    "    data, labels, val_size=0.2, test_size=0.1, random_state=101)\n",
    "print(\"Train: \", X_train.shape, y_train.shape)\n",
    "print(\"Validation: \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete temporary variables to free some memory\n",
    "del df, data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Vocabulary in notes:', 22476)\n",
      "('Vocabulary in original embedding:', 400000)\n",
      "('Vocabulary intersection:', 14345)\n"
     ]
    }
   ],
   "source": [
    "#creating embeddings\n",
    "EMBEDDING_LOC = '../data/glove.6B.100d.txt' # location of embedding\n",
    "EMBEDDING_DIM = 100 # given the glove that we chose\n",
    "embedding_matrix, embedding_dict = vectorization.embedding_matrix(EMBEDDING_LOC,\n",
    "                                                                  dictionary, EMBEDDING_DIM, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-d0b9af64ff1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN for text classification\n",
    "\n",
    "Based on the following papers and links:\n",
    "* \"Convolutional Neural Networks for Sentence Classification\"   \n",
    "* \"A Sensitivity Analysis of (and Practitionersâ€™ Guide to) Convolutional Neural Networks for Sentence Classification\"\n",
    "* http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "* https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras/blob/master/sentiment_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### set parameters:\n",
    "num_filters = 100\n",
    "filter_sizes = [2,3,4,5]\n",
    "training_dropout_keep_prob = 0.9\n",
    "num_classes=20\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "external_embeddings = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 699 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "29s - loss: 2.7167 - acc: 0.8148 - val_loss: 2.9682 - val_acc: 0.8123\n",
      "Epoch 2/5\n",
      "29s - loss: 2.8413 - acc: 0.8168 - val_loss: 2.9271 - val_acc: 0.8078\n",
      "Epoch 3/5\n",
      "29s - loss: 2.7164 - acc: 0.8168 - val_loss: 2.7181 - val_acc: 0.8078\n",
      "Epoch 4/5\n",
      "29s - loss: 2.4198 - acc: 0.8166 - val_loss: 2.4163 - val_acc: 0.8063\n",
      "Epoch 5/5\n",
      "29s - loss: 2.0446 - acc: 0.8278 - val_loss: 1.7713 - val_acc: 0.8210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f71d4df6d90>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Embedding\n",
    "if external_embeddings:\n",
    "    # use embedding_matrix\n",
    "    model_input = Input(shape= (MAX_SEQ_LENGTH, EMBEDDING_DIM))\n",
    "    X_train_input = np.stack([np.stack([embedding_matrix[word_id] for word_id in x]) for x in X_train])\n",
    "    X_val_input = np.stack([np.stack([embedding_matrix[word_id] for word_id in x]) for x in X_val])\n",
    "    z = model_input\n",
    "else:\n",
    "    # train embeddings\n",
    "    model_input = Input(shape=(MAX_SEQ_LENGTH, ))\n",
    "    X_train_input = X_train\n",
    "    X_val_input = X_val \n",
    "    z =  Embedding(MAX_VOCAB + 1, \n",
    "                   EMBEDDING_DIM, \n",
    "                   input_length=MAX_SEQ_LENGTH, \n",
    "                   name=\"embedding\")(model_input)\n",
    "\n",
    "# Convolutional block\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    window_pool_size =  MAX_SEQ_LENGTH  - sz + 1 \n",
    "    conv = MaxPooling1D(pool_size=2)(conv)  #pool_size?\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "#concatenate\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "z = Dropout(training_dropout_keep_prob)(z)\n",
    "\n",
    "#score prediction\n",
    "#z = Dense(num_classes, activation=\"relu\")(z)  I don't think this is necessary\n",
    "model_output = Dense(num_classes, activation=\"softmax\")(z)\n",
    "\n",
    "#creating model\n",
    "model = Model(model_input, model_output)\n",
    "# what to use for tf.nn.softmax_cross_entropy_with_logits?\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_input, y_train, batch_size=batch_size, epochs=epochs,\n",
    "validation_data=(X_val_input, y_val), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results with external embeddings = True\n",
    "```\n",
    "Train on 699 samples, validate on 200 samples\n",
    "Epoch 1/5\n",
    "29s - loss: 2.7167 - acc: 0.8148 - val_loss: 2.9682 - val_acc: 0.8123\n",
    "Epoch 2/5\n",
    "29s - loss: 2.8413 - acc: 0.8168 - val_loss: 2.9271 - val_acc: 0.8078\n",
    "Epoch 3/5\n",
    "29s - loss: 2.7164 - acc: 0.8168 - val_loss: 2.7181 - val_acc: 0.8078\n",
    "Epoch 4/5\n",
    "29s - loss: 2.4198 - acc: 0.8166 - val_loss: 2.4163 - val_acc: 0.8063\n",
    "Epoch 5/5\n",
    "29s - loss: 2.0446 - acc: 0.8278 - val_loss: 1.7713 - val_acc: 0.8210\n",
    "```\n",
    "\n",
    "### Results with external embeddings = False\n",
    "```\n",
    "Train on 699 samples, validate on 200 samples\n",
    "Epoch 1/5\n",
    "37s - loss: 0.9028 - acc: 0.8270 - val_loss: 0.5943 - val_acc: 0.8238\n",
    "Epoch 2/5\n",
    "36s - loss: 0.5272 - acc: 0.8320 - val_loss: 0.5536 - val_acc: 0.8238\n",
    "Epoch 3/5\n",
    "36s - loss: 0.5040 - acc: 0.8320 - val_loss: 0.5544 - val_acc: 0.8238\n",
    "Epoch 4/5\n",
    "35s - loss: 0.4820 - acc: 0.8320 - val_loss: 0.5517 - val_acc: 0.8238\n",
    "Epoch 5/5\n",
    "35s - loss: 0.4518 - acc: 0.8323 - val_loss: 0.5523 - val_acc: 0.8238\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train_input, batch_size=50)\n",
    "pred_dev = model.predict(X_val_input, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores\n",
      "threshold | training | dev  \n",
      "0.020:      0.326      0.314\n",
      "0.030:      0.329      0.303\n",
      "0.040:      0.332      0.298\n",
      "0.050:      0.331      0.292\n",
      "0.055:      0.330      0.288\n",
      "0.058:      0.327      0.289\n",
      "0.060:      0.326      0.289\n",
      "0.080:      0.320      0.277\n",
      "0.100:      0.316      0.265\n",
      "0.500:      0.211      0.197\n"
     ]
    }
   ],
   "source": [
    "def get_f1_score(y_true,y_hat,threshold, average):\n",
    "    hot_y = np.where(np.array(y_hat) > threshold, 1, 0)\n",
    "    return f1_score(np.array(y_true), hot_y, average=average)\n",
    "\n",
    "print 'F1 scores'\n",
    "print 'threshold | training | dev  '\n",
    "f1_score_average = 'micro'\n",
    "for threshold in [ 0.02, 0.03,0.04,0.05,0.055,0.058,0.06, 0.08, 0.1, 0.5]:\n",
    "    train_f1 = get_f1_score(y_train, pred_train,threshold,f1_score_average)\n",
    "    dev_f1 = get_f1_score(y_val, pred_dev,threshold,f1_score_average)\n",
    "    print '%1.3f:      %1.3f      %1.3f' % (threshold,train_f1, dev_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results with external embeddings = True \n",
    "```\n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.020:      0.326      0.314\n",
    "0.030:      0.329      0.303\n",
    "0.040:      0.332      0.298\n",
    "0.050:      0.331      0.292\n",
    "0.055:      0.330      0.288\n",
    "0.058:      0.327      0.289\n",
    "0.060:      0.326      0.289\n",
    "0.080:      0.320      0.277\n",
    "0.100:      0.316      0.265\n",
    "0.500:      0.211      0.197\n",
    "```\n",
    "\n",
    "### Results with external embeddings = False\n",
    "```\n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.030:      0.462      0.345\n",
    "0.040:      0.557      0.367\n",
    "0.050:      0.604      0.386\n",
    "0.055:      0.599      0.386\n",
    "0.058:      0.592      0.391\n",
    "0.060:      0.591      0.392\n",
    "0.080:      0.574      0.378\n",
    "0.100:      0.543      0.343\n",
    "0.500:      0.003      0.000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
