{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Custom functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import database_selection\n",
    "import vectorization\n",
    "import helpers\n",
    "\n",
    "#keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/disch_notes_all_icd9.csv',\n",
    "                 names = ['HADM_ID', 'SUBJECT_ID', 'DATE', 'ICD9','TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_TOP = 20 \n",
    "full_df, top_codes = database_selection.filter_top_codes(df, 'ICD9', N_TOP, filter_empty = True)\n",
    "df = full_df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preprocess icd9 codes\n",
    "labels = vectorization.vectorize_icd_column(df, 'ICD9', top_codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zenla/anaconda2/lib/python2.7/site-packages/pandas/core/generic.py:2773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 22476\n",
      "Average note length: 1748.878\n",
      "Max note length: 5641\n",
      "Final Vocabulary: 22476\n",
      "Final Max Sequence Length: 5000\n"
     ]
    }
   ],
   "source": [
    "#preprocess notes\n",
    "MAX_VOCAB = None # to limit original number of words (None if no limit)\n",
    "MAX_SEQ_LENGTH = 5000 # to limit length of word sequence (None if no limit)\n",
    "df.TEXT = vectorization.clean_notes(df, 'TEXT')\n",
    "data, dictionary, MAX_VOCAB = vectorization.vectorize_notes(df.TEXT, MAX_VOCAB, verbose = True)\n",
    "data, MAX_SEQ_LENGTH = vectorization.pad_notes(data, MAX_SEQ_LENGTH)\n",
    "print(\"Final Vocabulary: %s\" % MAX_VOCAB)\n",
    "print(\"Final Max Sequence Length: %s\" % MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train: ', (699, 5000), (699, 20))\n",
      "('Validation: ', (200, 5000), (200, 20))\n",
      "('Test: ', (101, 5000), (101, 20))\n"
     ]
    }
   ],
   "source": [
    "#split sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = helpers.train_val_test_split(\n",
    "    data, labels, val_size=0.2, test_size=0.1, random_state=101)\n",
    "print(\"Train: \", X_train.shape, y_train.shape)\n",
    "print(\"Validation: \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete temporary variables to free some memory\n",
    "del df, data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Vocabulary in notes:', 22476)\n",
      "('Vocabulary in original embedding:', 400000)\n",
      "('Vocabulary intersection:', 14345)\n"
     ]
    }
   ],
   "source": [
    "#creating embeddings\n",
    "EMBEDDING_LOC = '../data/glove.6B.100d.txt' # location of embedding\n",
    "EMBEDDING_DIM = 100 # given the glove that we chose\n",
    "embedding_matrix, embedding_dict = vectorization.embedding_matrix(EMBEDDING_LOC,\n",
    "                                                                  dictionary, EMBEDDING_DIM, verbose = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN for text classification\n",
    "\n",
    "Based on the following papers and links:\n",
    "* \"Convolutional Neural Networks for Sentence Classification\"   \n",
    "* \"A Sensitivity Analysis of (and Practitionersâ€™ Guide to) Convolutional Neural Networks for Sentence Classification\"\n",
    "* http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "* https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras/blob/master/sentiment_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### set parameters:\n",
    "num_filters = 100\n",
    "filter_sizes = [2,3,4,5]\n",
    "training_dropout_keep_prob = 0.9\n",
    "num_classes=N_TOP\n",
    "batch_size = 50\n",
    "epochs = 5\n",
    "external_embeddings = True\n",
    "EMBEDDING_TRAINABLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 699 samples, validate on 200 samples\n",
      "Epoch 1/5\n",
      "25s - loss: 1.9658 - acc: 0.8042 - val_loss: 0.6713 - val_acc: 0.8238\n",
      "Epoch 2/5\n",
      "26s - loss: 1.7716 - acc: 0.8065 - val_loss: 0.7155 - val_acc: 0.8212\n",
      "Epoch 3/5\n",
      "25s - loss: 1.5525 - acc: 0.8109 - val_loss: 0.6016 - val_acc: 0.8240\n",
      "Epoch 4/5\n",
      "25s - loss: 1.3251 - acc: 0.8132 - val_loss: 0.5844 - val_acc: 0.8238\n",
      "Epoch 5/5\n",
      "25s - loss: 1.1638 - acc: 0.8148 - val_loss: 0.5617 - val_acc: 0.8238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdb90775f10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Embedding\n",
    "model_input = Input(shape=(MAX_SEQ_LENGTH, ))\n",
    "if external_embeddings:\n",
    "    # use embedding_matrix plus local training\n",
    "    z = Embedding(MAX_VOCAB + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQ_LENGTH,\n",
    "                            trainable=EMBEDDING_TRAINABLE)(model_input)\n",
    "else:\n",
    "    # train embeddings \n",
    "    z =  Embedding(MAX_VOCAB + 1, \n",
    "                   EMBEDDING_DIM, \n",
    "                   input_length=MAX_SEQ_LENGTH, \n",
    "                   name=\"embedding\")(model_input)\n",
    "\n",
    "# Convolutional block\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    window_pool_size =  MAX_SEQ_LENGTH  - sz + 1 \n",
    "    conv = MaxPooling1D(pool_size=window_pool_size)(conv)  \n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "\n",
    "#concatenate\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "z = Dropout(training_dropout_keep_prob)(z)\n",
    "\n",
    "#score prediction\n",
    "#z = Dense(num_classes, activation=\"relu\")(z)  I don't think this is necessary\n",
    "model_output = Dense(num_classes, activation=\"softmax\")(z)\n",
    "\n",
    "#creating model\n",
    "model = Model(model_input, model_output)\n",
    "# what to use for tf.nn.softmax_cross_entropy_with_logits?\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "validation_data=(X_val, y_val), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train, batch_size=50)\n",
    "pred_dev = model.predict(X_val, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores\n",
      "threshold | training | dev  \n",
      "0.020:      0.334      0.333\n",
      "0.030:      0.362      0.360\n",
      "0.040:      0.366      0.374\n",
      "0.050:      0.373      0.380\n",
      "0.055:      0.374      0.382\n",
      "0.058:      0.376      0.376\n",
      "0.060:      0.376      0.378\n",
      "0.080:      0.387      0.371\n",
      "0.100:      0.366      0.350\n",
      "0.200:      0.179      0.171\n",
      "0.300:      0.020      0.020\n",
      "0.500:      0.000      0.000\n"
     ]
    }
   ],
   "source": [
    "def get_f1_score(y_true,y_hat,threshold, average):\n",
    "    hot_y = np.where(np.array(y_hat) > threshold, 1, 0)\n",
    "    return f1_score(np.array(y_true), hot_y, average=average)\n",
    "\n",
    "print 'F1 scores'\n",
    "print 'threshold | training | dev  '\n",
    "f1_score_average = 'micro'\n",
    "for threshold in [ 0.02, 0.03,0.04,0.05,0.055,0.058,0.06, 0.08, 0.1,0.2,0.3, 0.5]:\n",
    "    train_f1 = get_f1_score(y_train, pred_train,threshold,f1_score_average)\n",
    "    dev_f1 = get_f1_score(y_val, pred_dev,threshold,f1_score_average)\n",
    "    print '%1.3f:      %1.3f      %1.3f' % (threshold,train_f1, dev_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results with external embeddings = True , no additional training,  top 20\n",
    "```\n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.020:      0.337      0.329\n",
    "0.030:      0.360      0.353\n",
    "0.040:      0.365      0.374\n",
    "0.050:      0.372      0.375\n",
    "0.055:      0.370      0.377\n",
    "0.058:      0.369      0.375\n",
    "0.060:      0.368      0.375\n",
    "0.080:      0.348      0.361\n",
    "0.100:      0.309      0.319\n",
    "0.200:      0.198      0.208\n",
    "0.300:      0.157      0.138\n",
    "0.500:      0.000      0.000\n",
    "```\n",
    "\n",
    "### Results with external embeddings = False, top 20\n",
    "```\n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.030:      0.462      0.345\n",
    "0.040:      0.557      0.367\n",
    "0.050:      0.604      0.386\n",
    "0.055:      0.599      0.386\n",
    "0.058:      0.592      0.391\n",
    "0.060:      0.591      0.392\n",
    "0.080:      0.574      0.378\n",
    "0.100:      0.543      0.343\n",
    "0.500:      0.003      0.000\n",
    "```\n",
    "\n",
    "### Results with external embedding and training them , top 20\n",
    "```\n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.020:      0.334      0.333\n",
    "0.030:      0.362      0.360\n",
    "0.040:      0.366      0.374\n",
    "0.050:      0.373      0.380\n",
    "0.055:      0.374      0.382\n",
    "0.058:      0.376      0.376\n",
    "0.060:      0.376      0.378\n",
    "0.080:      0.387      0.371\n",
    "0.100:      0.366      0.350\n",
    "0.200:      0.179      0.171\n",
    "0.300:      0.020      0.020\n",
    "0.500:      0.000      0.000\n",
    "\n",
    "```\n",
    "\n",
    "### Results with external Embeddings = False, top 10, \n",
    "We can compare this setup with the LSTM published in the paper \"Applying Deep Learning to ICD-9 Multi-label Classification from Medical Records\", they got a F1-score of about 0.4168, we are getting 0.447\n",
    "\n",
    "``` \n",
    "F1 scores\n",
    "threshold | training | dev  \n",
    "0.020:      0.444      0.409\n",
    "0.030:      0.491      0.413\n",
    "0.040:      0.554      0.426\n",
    "0.050:      0.611      0.433\n",
    "0.055:      0.642      0.437\n",
    "0.058:      0.658      0.438\n",
    "0.060:      0.669      0.434\n",
    "0.080:      0.783      0.441\n",
    "0.100:      0.867      0.447\n",
    "0.300:      0.400      0.116\n",
    "0.500:      0.095      0.004\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
