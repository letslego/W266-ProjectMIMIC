{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## just a workbook, we will add more comments in the notebook for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "\n",
    "\n",
    "#keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "# Custom functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import database_selection\n",
    "import vectorization\n",
    "import helpers\n",
    "import icd9_cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading file\n",
    "full_df = pd.read_csv('../data/disch_notes_all_icd9.csv',\n",
    "                 names = ['HADM_ID', 'SUBJECT_ID', 'DATE', 'ICD9','TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5270, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>ICD9</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156816</td>\n",
       "      <td>2251</td>\n",
       "      <td>2189-06-27 00:00:00</td>\n",
       "      <td>4241 4019 53081 2449</td>\n",
       "      <td>Admission Date:  [**2189-6-22**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>190759</td>\n",
       "      <td>32680</td>\n",
       "      <td>2152-05-26 00:00:00</td>\n",
       "      <td>34120 5990 99812 2800 2869 4019 2720 3682 92232</td>\n",
       "      <td>Admission Date:  [**2152-5-10**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181175</td>\n",
       "      <td>27162</td>\n",
       "      <td>2171-09-03 00:00:00</td>\n",
       "      <td>431 5856 3484 48283 40391 75313 34292 00845 41...</td>\n",
       "      <td>Admission Date:  [**2171-8-15**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>160651</td>\n",
       "      <td>72541</td>\n",
       "      <td>2192-06-23 00:00:00</td>\n",
       "      <td>2724 81002 81109 80130 3485 80718 9515 9514 78...</td>\n",
       "      <td>Admission Date:  [**2192-6-16**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176849</td>\n",
       "      <td>6596</td>\n",
       "      <td>2107-02-26 00:00:00</td>\n",
       "      <td>769 76519</td>\n",
       "      <td>Admission Date:  [**2107-2-24**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>133975</td>\n",
       "      <td>31160</td>\n",
       "      <td>2184-10-12 00:00:00</td>\n",
       "      <td>486 0389 42823 78552 78551 5849 4271 99731 428...</td>\n",
       "      <td>Admission Date:  [**2184-9-27**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>180297</td>\n",
       "      <td>32452</td>\n",
       "      <td>2111-12-25 00:00:00</td>\n",
       "      <td>74685 74190 4430 4139</td>\n",
       "      <td>Admission Date:  [**2111-12-15**]             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>112656</td>\n",
       "      <td>16732</td>\n",
       "      <td>2137-01-08 00:00:00</td>\n",
       "      <td>44024 9971 4280 496 70715 45829 2930 25060 3572</td>\n",
       "      <td>Admission Date:  [**2136-12-30**]       Discha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>177462</td>\n",
       "      <td>28621</td>\n",
       "      <td>2125-09-10 00:00:00</td>\n",
       "      <td>9654 570 29590 5845 2760 2967</td>\n",
       "      <td>Admission Date:  [**2125-8-29**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>144109</td>\n",
       "      <td>27710</td>\n",
       "      <td>2127-11-10 00:00:00</td>\n",
       "      <td>1983 1970 3314</td>\n",
       "      <td>Admission Date:  [**2127-11-1**]              ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID  SUBJECT_ID                 DATE  \\\n",
       "0   156816        2251  2189-06-27 00:00:00   \n",
       "1   190759       32680  2152-05-26 00:00:00   \n",
       "2   181175       27162  2171-09-03 00:00:00   \n",
       "3   160651       72541  2192-06-23 00:00:00   \n",
       "4   176849        6596  2107-02-26 00:00:00   \n",
       "5   133975       31160  2184-10-12 00:00:00   \n",
       "6   180297       32452  2111-12-25 00:00:00   \n",
       "7   112656       16732  2137-01-08 00:00:00   \n",
       "8   177462       28621  2125-09-10 00:00:00   \n",
       "9   144109       27710  2127-11-10 00:00:00   \n",
       "\n",
       "                                                ICD9  \\\n",
       "0                               4241 4019 53081 2449   \n",
       "1    34120 5990 99812 2800 2869 4019 2720 3682 92232   \n",
       "2  431 5856 3484 48283 40391 75313 34292 00845 41...   \n",
       "3  2724 81002 81109 80130 3485 80718 9515 9514 78...   \n",
       "4                                          769 76519   \n",
       "5  486 0389 42823 78552 78551 5849 4271 99731 428...   \n",
       "6                              74685 74190 4430 4139   \n",
       "7    44024 9971 4280 496 70715 45829 2930 25060 3572   \n",
       "8                      9654 570 29590 5845 2760 2967   \n",
       "9                                     1983 1970 3314   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2189-6-22**]       Dischar...  \n",
       "1  Admission Date:  [**2152-5-10**]              ...  \n",
       "2  Admission Date:  [**2171-8-15**]              ...  \n",
       "3  Admission Date:  [**2192-6-16**]              ...  \n",
       "4  Admission Date:  [**2107-2-24**]       Dischar...  \n",
       "5  Admission Date:  [**2184-9-27**]              ...  \n",
       "6  Admission Date:  [**2111-12-15**]             ...  \n",
       "7  Admission Date:  [**2136-12-30**]       Discha...  \n",
       "8  Admission Date:  [**2125-8-29**]              ...  \n",
       "9  Admission Date:  [**2127-11-1**]              ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking just a subset of the records for developing models\n",
    "df = full_df.sample(frac=0.1).reset_index(drop=True)\n",
    "print df.shape\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing ICD 9 codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>ICD9</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156816</td>\n",
       "      <td>2251</td>\n",
       "      <td>2189-06-27 00:00:00</td>\n",
       "      <td>240-279 390-459 520-579</td>\n",
       "      <td>Admission Date:  [**2189-6-22**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>190759</td>\n",
       "      <td>32680</td>\n",
       "      <td>2152-05-26 00:00:00</td>\n",
       "      <td>240-279 390-459 290-319 800-999 320-389 580-629</td>\n",
       "      <td>Admission Date:  [**2152-5-10**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181175</td>\n",
       "      <td>27162</td>\n",
       "      <td>2171-09-03 00:00:00</td>\n",
       "      <td>240-279 760-779 001-139 390-459 290-319 460-51...</td>\n",
       "      <td>Admission Date:  [**2171-8-15**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>160651</td>\n",
       "      <td>72541</td>\n",
       "      <td>2192-06-23 00:00:00</td>\n",
       "      <td>240-279 390-459 460-519 800-999 780-789 320-389</td>\n",
       "      <td>Admission Date:  [**2192-6-16**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176849</td>\n",
       "      <td>6596</td>\n",
       "      <td>2107-02-26 00:00:00</td>\n",
       "      <td>760-779</td>\n",
       "      <td>Admission Date:  [**2107-2-24**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>133975</td>\n",
       "      <td>31160</td>\n",
       "      <td>2184-10-12 00:00:00</td>\n",
       "      <td>240-279 001-139 390-459 460-519 800-999 780-78...</td>\n",
       "      <td>Admission Date:  [**2184-9-27**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>180297</td>\n",
       "      <td>32452</td>\n",
       "      <td>2111-12-25 00:00:00</td>\n",
       "      <td>390-459 760-779</td>\n",
       "      <td>Admission Date:  [**2111-12-15**]             ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>112656</td>\n",
       "      <td>16732</td>\n",
       "      <td>2137-01-08 00:00:00</td>\n",
       "      <td>240-279 680-709 390-459 290-319 460-519 800-99...</td>\n",
       "      <td>Admission Date:  [**2136-12-30**]       Discha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>177462</td>\n",
       "      <td>28621</td>\n",
       "      <td>2125-09-10 00:00:00</td>\n",
       "      <td>580-629 240-279 290-319 520-579 800-999</td>\n",
       "      <td>Admission Date:  [**2125-8-29**]              ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>144109</td>\n",
       "      <td>27710</td>\n",
       "      <td>2127-11-10 00:00:00</td>\n",
       "      <td>140-239 320-389</td>\n",
       "      <td>Admission Date:  [**2127-11-1**]              ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID  SUBJECT_ID                 DATE  \\\n",
       "0   156816        2251  2189-06-27 00:00:00   \n",
       "1   190759       32680  2152-05-26 00:00:00   \n",
       "2   181175       27162  2171-09-03 00:00:00   \n",
       "3   160651       72541  2192-06-23 00:00:00   \n",
       "4   176849        6596  2107-02-26 00:00:00   \n",
       "5   133975       31160  2184-10-12 00:00:00   \n",
       "6   180297       32452  2111-12-25 00:00:00   \n",
       "7   112656       16732  2137-01-08 00:00:00   \n",
       "8   177462       28621  2125-09-10 00:00:00   \n",
       "9   144109       27710  2127-11-10 00:00:00   \n",
       "\n",
       "                                                ICD9  \\\n",
       "0                            240-279 390-459 520-579   \n",
       "1    240-279 390-459 290-319 800-999 320-389 580-629   \n",
       "2  240-279 760-779 001-139 390-459 290-319 460-51...   \n",
       "3    240-279 390-459 460-519 800-999 780-789 320-389   \n",
       "4                                            760-779   \n",
       "5  240-279 001-139 390-459 460-519 800-999 780-78...   \n",
       "6                                    390-459 760-779   \n",
       "7  240-279 680-709 390-459 290-319 460-519 800-99...   \n",
       "8            580-629 240-279 290-319 520-579 800-999   \n",
       "9                                    140-239 320-389   \n",
       "\n",
       "                                                TEXT  \n",
       "0  Admission Date:  [**2189-6-22**]       Dischar...  \n",
       "1  Admission Date:  [**2152-5-10**]              ...  \n",
       "2  Admission Date:  [**2171-8-15**]              ...  \n",
       "3  Admission Date:  [**2192-6-16**]              ...  \n",
       "4  Admission Date:  [**2107-2-24**]       Dischar...  \n",
       "5  Admission Date:  [**2184-9-27**]              ...  \n",
       "6  Admission Date:  [**2111-12-15**]             ...  \n",
       "7  Admission Date:  [**2136-12-30**]       Discha...  \n",
       "8  Admission Date:  [**2125-8-29**]              ...  \n",
       "9  Admission Date:  [**2127-11-1**]              ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ICD9_FIRST_LEVEL = [\n",
    "    '001-139','140-239','240-279','290-319', '320-389', '390-459','460-519', '520-579', '580-629', \n",
    "    '630-679', '680-709','710-739', '760-779', '780-789', '790-796', '797', '798', '799', '800-999' ]\n",
    "N_TOP = len(ICD9_FIRST_LEVEL)\n",
    "# replacing leave ICD9 codes with the grandparents\n",
    "df['ICD9'] = df['ICD9'].apply(lambda x: helpers.replace_with_grandparent_codes(x,ICD9_FIRST_LEVEL))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocess icd9 codes\n",
    "top_codes = ICD9_FIRST_LEVEL\n",
    "labels = vectorization.vectorize_icd_column(df, 'ICD9', top_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Notes\n",
    "\n",
    "The notes preprocessin here is a little different sice we want to keep dots and other characters to be able to split the notes into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "note_sentences = []\n",
    "notes = []\n",
    "\n",
    "for idx in range(df.shape[0]):\n",
    "    # for every note\n",
    "    text = clean_str(df[\"TEXT\"][idx].encode('ascii','ignore'))\n",
    "    notes.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    note_sentences.append(sentences)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admission date:  [**2189-6-22**]       discharge date:  [**2189-6-27**] date of birth:   [**2112-4-17**]       sex:  f service:  cardiothoracic service chief complaint:  ms [**known lastname 35085**] is a direct admit into the operating room where she will undergo minimally invasive aortic valve replacement.',\n",
       " 'her chief complaint is a worsening heart murmur.',\n",
       " 'hi[**last name (stitle) 2710**]of present illness:  this is a pleasant, asymptomatic, 77 year old woman with a history of heart murmur since her 40s without symptoms who has been followed by her primary care provider and cardiologist over the years with serial echocardiograms.',\n",
       " 'the patients echocardiogram last winter appeared worse than the previous so the patient was referred to dr. [**last name (prefixes) **] for aortic valve surgery.',\n",
       " 'she had cardiac catheterization done in [**2188-8-17**] which showed normal coronaries with severe aortic stenosis following a successful aortic valvuloplasty which increased her aortic valve area from 0.6 to 0.85.  cardiac echocardiogram done in [**2188-6-16**] showed an ejection fraction of 55% with a mildly dilated ascending aorta, aortic valve leaflets were moderately thickened with severe aortic stenosis, mild mitral regurgitation, borderline pulmonary hypertension.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note_sentences[0][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what will be the MAX_SENTS and MAX_SENT_LENGTH ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of sentences in a note:  104.830550285\n",
      "Max number of sentences in a note:  488\n"
     ]
    }
   ],
   "source": [
    "note_sentences_length =[len(x) for x in note_sentences]\n",
    "print \"Average number of sentences in a note: \", np.mean(note_sentences_length)  \n",
    "print \"Max number of sentences in a note: \", max(note_sentences_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(note_sentences_length) > 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words in a sentence:  16.969297158\n",
      "Max number of words in a sentence:  3841\n"
     ]
    }
   ],
   "source": [
    "sentence_flat_list = [sentence for note_sentence in note_sentences for sentence in note_sentence]\n",
    "sentence_words_length =[len(text_to_word_sequence(sentence)) for sentence in sentence_flat_list]\n",
    "print \"Average number of words in a sentence: \", np.mean(sentence_words_length)  \n",
    "print \"Max number of words in a sentence: \", max(sentence_words_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1678"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(sentence_words_length) > 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = None\n",
    "MAX_SENTS = 150\n",
    "MAX_SENT_LENGTH  = 250\n",
    "\n",
    "MAX_VOCAB = None # to limit original number of words (None if no limit)\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(notes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = len(tokenizer.word_index)  #vocabulary length\n",
    "note_matrix = np.zeros((len(notes), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "for i, one_note_sentences in enumerate(note_sentences):\n",
    "    for j, sentence in enumerate(one_note_sentences):\n",
    "        if j< MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sentence)\n",
    "            k=0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n",
    "                    note_matrix[i,j,k] = tokenizer.word_index[word]\n",
    "                    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   54,    56,  1146,    22,   177,    31,    56,  1146,    22,\n",
       "         179,    56,     3,   251,  1124,    17,   144,   280,   350,\n",
       "         121,  1296,   121,   353,   337,   777,   107,   235, 35016,\n",
       "          19,     8,  2191,  2801,   665,     1,   962,   333,   514,\n",
       "          30,   126,  3304,  2219,   340,   135,   138,   915,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note_matrix[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Vocabulary in notes:', 66712)\n",
      "('Vocabulary in original embedding:', 400000)\n",
      "('Vocabulary intersection:', 25296)\n"
     ]
    }
   ],
   "source": [
    "#creating embeddings\n",
    "EMBEDDING_LOC = '../data/glove.6B.100d.txt' # location of embedding\n",
    "EMBEDDING_DIM = 100 # given the glove that we chose\n",
    "EMBEDDING_MATRIX, embedding_dict = vectorization.embedding_matrix(EMBEDDING_LOC,\n",
    "                                                                  dictionary, EMBEDDING_DIM, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train: ', (3688, 150, 250), (3688, 19))\n",
      "('Validation: ', (1054, 150, 250), (1054, 19))\n",
      "('Test: ', (528, 150, 250), (528, 19))\n"
     ]
    }
   ],
   "source": [
    "#split sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = helpers.train_val_test_split(\n",
    "    note_matrix, labels, val_size=0.2, test_size=0.1, random_state=101)\n",
    "print(\"Train: \", X_train.shape, y_train.shape)\n",
    "print(\"Validation: \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Attention NN\n",
    "based on paper: Hierarchical Attention networks for document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hatt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hatt_model.py:29: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 250, 200)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 250)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 250, 100)      6671300     input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional)  (None, 250, 200)      120600      embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistribu (None, 250, 200)      40200       bidirectional_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "permute_5 (Permute)              (None, 200, 250)      0           time_distributed_7[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)              (None, 200, 250)      0           permute_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 200, 250)      62750       reshape_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "attention_vec (Permute)          (None, 250, 200)      0           dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "attention_mul (Merge)            (None, 250, 200)      0           time_distributed_7[0][0]         \n",
      "                                                                   attention_vec[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)                (None, 200)           0           attention_mul[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 6,894,850\n",
      "Trainable params: 6,894,850\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "(?, 150, 200)\n",
      "model fitting - Hierachical Attention GRU\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_6 (InputLayer)             (None, 150, 250)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistribu (None, 150, 200)      6894850     input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional)  (None, 150, 200)      180600      time_distributed_8[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistribu (None, 150, 200)      40200       bidirectional_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "permute_6 (Permute)              (None, 200, 150)      0           time_distributed_9[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)              (None, 200, 150)      0           permute_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 200, 150)      22650       reshape_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "attention_vec (Permute)          (None, 150, 200)      0           dense_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "attention_mul (Merge)            (None, 150, 200)      0           time_distributed_9[0][0]         \n",
      "                                                                   attention_vec[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)                (None, 200)           0           attention_mul[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "preds (Dense)                    (None, 19)            3819        lambda_6[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 7,142,119\n",
      "Trainable params: 7,142,119\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "reload(hatt_model)\n",
    "h_att_model = hatt_model.build_gru_att_model(MAX_SENTS, MAX_SENT_LENGTH, \n",
    "                         max_vocab=MAX_NB_WORDS, embedding_dim=EMBEDDING_DIM , embedding_matrix=EMBEDDING_MATRIX,\n",
    "                         num_classes=N_TOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3688 samples, validate on 1054 samples\n",
      "Epoch 1/1\n",
      "3688/3688 [==============================] - 3534s - loss: 0.7549 - acc: 0.7228 - val_loss: 0.7396 - val_acc: 0.7249\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5612c87f50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "h_att_model.fit(X_train, y_train, batch_size=50, epochs=1, validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_train = h_att_model.predict(X_train, batch_size=50)\n",
    "pred_dev = h_att_model.predict(X_val, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores\n",
      "threshold | training | dev  \n",
      "0.020:      0.556      0.551\n",
      "0.030:      0.579      0.574\n",
      "0.040:      0.585      0.579\n",
      "0.050:      0.597      0.591\n",
      "0.055:      0.601      0.593\n",
      "0.058:      0.602      0.594\n",
      "0.060:      0.608      0.602\n",
      "0.080:      0.563      0.558\n",
      "0.100:      0.529      0.522\n",
      "0.200:      0.000      0.000\n",
      "0.300:      0.000      0.000\n",
      "0.500:      0.000      0.000\n"
     ]
    }
   ],
   "source": [
    "helpers.show_f1_score(y_train, pred_train, y_val, pred_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes:\n",
    "* This is looking good! with just ONE Epoch, we get a F1score of 60%, almost the same than 5 Epochs with the CNN mocdel (f1=63%)\n",
    "* It was super slow though, that is why I didn't run the 5 epcchs.. it took like 2 hours to run 1 epoch.. I'll work on improving that (hopefully it can be improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
